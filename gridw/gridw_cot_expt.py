import re
import numpy as np
import os
import json
import shutil
import collections

import arg_util
import run_eval
import log_util
import gridw.gridw_util as grid_util
import gridw.gridw_expt  # for eval_logs, get_point, etc.

#############################################
# 1) create_new_demo – now we pass in "args"
#############################################
def create_new_demo(example, grid, args):
    """Creates a chain-of-thought demonstration from a failed example.

    Analyzes the failure using gridw_expt and grid_util infrastructure:
      - If 'extract_start' fails (PTP parse fails), we fallback to CoT parse with regex
      - Then we see if it's an obstacle or suboptimal path error
      - Return a textual chain-of-thought snippet
    """
    # 1) Try PTP style parse
    start = gridw.gridw_expt.get_point('extract_start', example['output'])
    goal  = gridw.gridw_expt.get_point('extract_goal', example['output'])

    # 2) If that fails (-1 in start or goal), fallback to CoT parse with regex
    if -1 in start or -1 in goal:
        for line in example['output']:
            # typical pattern: "Move east from (3,4) to (4,4)"
            m = re.search(r"from .*?(\d+),(\d+)", line)
            if m:
                start = (int(m.group(1)), int(m.group(2)))
            m = re.search(r"to .*?(\d+),(\d+)", line)
            if m:
                goal = (int(m.group(1)), int(m.group(2)))

    # If we STILL can't parse properly, fallback or guess
    if -1 in start or -1 in goal:
        start = (0, 0)
        goal  = (7, 7)

    # 3) Identify correct vs. predicted path
    task = grid_util.TaskExample(start[0], start[1], goal[0], goal[1], grid)
    optimal_path = [a.name for a in task.target_path]
    predicted_path = example['y_hat'].split()

    # 4) Evaluate - (NEW) pass "args" so it can read disable_excess_len, etc
    distance_mat = grid_util.distance_matrix(grid, goal[0], goal[1])
    instance = gridw.gridw_expt.Instance(start[0], start[1], goal[0], goal[1], distance_mat)
    eval_result = gridw.gridw_expt.eval_path(
        predicted_path, 
        optimal_path, 
        instance,
        grid,
        args  # (NEW) pass actual args
    )

    # 5) Build chain-of-thought lines
    reasoning = []
    reasoning.append(f"Start at location ({start[0]},{start[1]}); goal is ({goal[0]},{goal[1]})")

    if eval_result.obstacles_crossed > 0 or eval_result.off_grid_moves > 0:
        reasoning.append("The predicted path collided with an obstacle or went off-grid. Let's analyze each step:")
        x, y = start
        for move in predicted_path:
            dx, dy = grid_util.DELTAS.get(move, (None, None))
            if dx is None or dy is None:
                reasoning.append(f"Move '{move}' not recognized (should be north/south/east/west).")
                break
            nx, ny = x + dx, y + dy
            if nx<0 or ny<0 or nx>=grid.shape[0] or ny>=grid.shape[1]:
                reasoning.append(f"Moving {move} from ({x},{y}) is off-grid.")
                break
            if grid[nx, ny]:
                reasoning.append(f"Moving {move} from ({x},{y}) hits an obstacle at ({nx},{ny}).")
                break
            x, y = nx, ny

        reasoning.append("\nHere's the safe, correct path:")
        for action in task.target_path:
            reasoning.append(f"  Move {action.name} from ({action.x0},{action.y0}) to ({action.x1},{action.y1})")

    elif eval_result.excess_path_len > 0:
        reasoning.append(
            f"The predicted path has {len(predicted_path)} steps, but the optimal path has {len(optimal_path)}."
        )
        reasoning.append("To be optimal, each step should reduce distance to the goal.")
        reasoning.append("\nCorrect path:")
        for action in task.target_path:
            reasoning.append(f"  Move {action.name} from ({action.x0},{action.y0}) to ({action.x1},{action.y1})")

    else:
        reasoning.append("Likely parse or unknown error. The correct route is below:")
        for action in task.target_path:
            reasoning.append(f"  Move {action.name} from ({action.x0},{action.y0}) to ({action.x1},{action.y1})")

    # Return snippet
    snippet = [
        f"\nQ: {example['input']}",
        "Let's think step by step:",
        "\n".join(reasoning),
        f"Final answer: {' '.join(optimal_path)}\n"
    ]
    return "\n".join(snippet)


def append_new_demos(basefile, new_snippets):
    with open(basefile, 'a', encoding="utf-8") as fp:
        for snip in new_snippets:
            fp.write(snip)
    print(f"Appended {len(new_snippets)} new demos to {basefile}")


def main():
    parser = arg_util.baseparser()

    parser.add_argument("--cot_json", default=None, help="Path to a JSON file containing the 'grid' array for CoT-based tasks.")
    parser.add_argument("--expt_lo", type=int, default=0, help="Start index of first training batch")
    parser.add_argument("--expt_hi", type=int, default=3, help="End index of first training batch (exclusive)")
    parser.add_argument("--num_iterations", type=int, default=1, help="Number of training iterations")
    parser.add_argument("--force", action="store_true", help="Re-run even if logs exist")
    parser.add_argument("--summarize_eval", action="store_true", help="Evaluate on test set after final iteration")

    args = parser.parse_args()
    arg_util.apply_shortcuts(args)

    # guess cot_json if missing
    if not args.cot_json:
        guessed_json = f"gridw/cot-prompts/{args.task}.json"
        print(f"No --cot_json supplied, guessing {guessed_json}")
        args.cot_json = guessed_json
    
    with open(args.cot_json, 'r') as f:
        data = json.load(f)
        grid = np.array(data['grid'])

    logfilename = arg_util.log_file(args)
    logdir = os.path.dirname(logfilename)
    accum_dir = os.path.join(logdir, "gridw_cot_expt_prompts")
    os.makedirs(accum_dir, exist_ok=True)

    def subdir(lo, hi):
        d = os.path.join(accum_dir, f"subdir_{lo:03d}_{hi:03d}")
        os.makedirs(d, exist_ok=True)
        return d

    batch_size = args.expt_hi - args.expt_lo
    lo = args.expt_lo
    hi = args.expt_hi

    for iter_idx in range(args.num_iterations):
        print(f"\n=== Iteration {iter_idx+1}/{args.num_iterations}, batch {lo}..{hi} ===")
        sub = subdir(lo, hi)
        iteration_template = os.path.join(sub, os.path.basename(args.template_file))

        # Copy from the base prompt or last iteration's prompt
        if iter_idx == 0:
            shutil.copyfile(args.template_file, iteration_template)
        else:
            prev_sub = subdir(lo - batch_size, hi - batch_size)
            prev_template = os.path.join(prev_sub, os.path.basename(args.template_file))
            shutil.copyfile(prev_template, iteration_template)

        # (NEW) Ensure the final lines contain "Question: {input_str}"
        with open(iteration_template, "r", encoding="utf-8") as fp:
            content = fp.read()
        if "{input_str}" not in content:
            # Append to the end
            # Possibly you want "Q: {input_str}" or "Question: {input_str}"
            content += "\n\nQuestion: {input_str}\nAnswer:\n"
            with open(iteration_template, "w", encoding="utf-8") as fp:
                fp.write(content)

        # Evaluate on [lo..hi]
        args.lo, args.hi = lo, hi
        args.template_file = iteration_template
        args.CoT = True

        batch_logfile = arg_util.log_file(args)
        if args.force or not os.path.exists(batch_logfile):
            print(f"Running run_eval on training examples {lo}..{hi}")
            run_eval.main(args)
        else:
            print(f"Reusing existing batch log: {batch_logfile}")

        # Identify mistakes
        batch_examples = log_util.load_examples(batch_logfile, verbose=False)
        mistakes = [ex for ex in batch_examples if not ex['is_correct']]

        # Create new CoT demos – pass args
        new_demos = []
        for ex in mistakes:
            snip = create_new_demo(ex, grid=grid, args=args)
            new_demos.append(snip)

        if new_demos:
            append_new_demos(iteration_template, new_demos)

        # Next iteration
        lo += batch_size
        hi += batch_size

    if args.summarize_eval:
        final_sub = subdir(lo - batch_size, hi - batch_size)
        final_template = os.path.join(final_sub, os.path.basename(args.template_file))
        print(f"\nNow evaluating final CoT prompt = {final_template} on test set…")

        args.test_set = True
        args.lo, args.hi = 0, 0
        args.template_file = final_template
        args.log_stem = f"cot-test_after_{lo - batch_size}_{hi - batch_size}"

        run_eval.main(args)
        test_logfile = arg_util.log_file(args)

        # Evaluate path metrics
        ex_test = log_util.load_examples(test_logfile, verbose=False)
        evals = gridw.gridw_expt.eval_logs(grid, ex_test, args)
        all_corr = collections.defaultdict(list)
        gridw.gridw_expt.collect_corrections(all_corr, evals, args)
        summary = gridw.gridw_expt.eval_summary(evals, all_corr, args)

        print("\n==== Final Test Summary ====")
        for k, v in summary.items():
            print(f"{k}\t{v}")

        print("All done! End of CoT experiment.")


if __name__ == "__main__":
    main()

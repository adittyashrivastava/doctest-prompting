#!/usr/bin/env python3
"""
Statistical Analysis Script for Grid World Navigation Metrics
Performs appropriate statistical tests based on metric types.
"""

import pandas as pd
import numpy as np
from scipy import stats
from pathlib import Path
import json
import argparse
import logging
from typing import Dict, List, Tuple
import sys

METRIC_DESCRIPTIONS = {
    'excess_len': 'Extra moves beyond optimal path length: max(length(predicted) - length(optimal), 0)',
    'obst_crossed': 'Number of attempted moves into obstacle cells',
    'dist_to_goal': 'Manhattan distance from final position to goal: |x_final - x_goal| + |y_final - y_goal|',
    'acc': 'Fraction of examples where predicted path exactly matches optimal path',
    'off_grid_moves': 'Average number of attempted moves outside grid boundaries',
    'possible_action_examples': 'Count of examples with non-optimal action choices',
    'optimal_action_examples': 'Count of examples with optimal action choices'
}

def setup_logging():
    """Configure logging settings."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

def load_data(file_path: str) -> pd.DataFrame:
    """
    Load data from CSV or Excel file.
    
    Args:
        file_path: Path to input file
        
    Returns:
        Pandas DataFrame containing the data
    """
    file_ext = Path(file_path).suffix.lower()
    try:
        if file_ext == '.csv':
            return pd.read_csv(file_path)
        elif file_ext in ['.xlsx', '.xls']:
            return pd.read_excel(file_path)
        else:
            raise ValueError(f"Unsupported file format: {file_ext}")
    except Exception as e:
        raise ValueError(f"Error reading file: {str(e)}")

def analyze_proportions(p1: float, p2: float, n: int, alpha: float) -> Dict:
    """
    Analyze proportions using both Z-test and Fisher's exact test.
    
    Args:
        p1, p2: Proportions to compare
        n: Sample size
        alpha: Significance level
    
    Returns:
        Dictionary with test results
    """
    # Z-test for proportions
    p_pooled = (p1 * n + p2 * n) / (2 * n)
    se = np.sqrt(p_pooled * (1 - p_pooled) * (2/n))
    z_stat = (p1 - p2) / se
    p_value_z = 2 * (1 - stats.norm.cdf(abs(z_stat)))
    
    # Fisher's exact test
    count1 = int(round(p1 * n))
    count2 = int(round(p2 * n))
    table = [[count1, n-count1], [count2, n-count2]]
    odds_ratio, p_value_f = stats.fisher_exact(table)
    
    # Effect size (Cohen's h)
    h = 2 * (np.arcsin(np.sqrt(p1)) - np.arcsin(np.sqrt(p2)))
    if min(p1, p2) == 0:
        return {
        'z_test_p': p_value_z,
        'fisher_p': p_value_f,
        'effect_size': abs(h),
        'absolute_diff': abs(p1 - p2),
        'relative_diff' : 0,
        'significant': (p_value_z < alpha) or (p_value_f < alpha)
    }
    else:
        return {
        'z_test_p': p_value_z,
        'fisher_p': p_value_f,
        'effect_size': abs(h),
        'absolute_diff': abs(p1 - p2),
        'relative_diff': abs(p1 - p2)/min(p1, p2) * 100,
        'significant': (p_value_z < alpha) or (p_value_f < alpha)
        }

def analyze_continuous(vals1: np.ndarray, vals2: np.ndarray, alpha: float) -> Dict:
    """
    Analyze continuous data using Mann-Whitney U test.
    
    Args:
        vals1, vals2: Arrays of values to compare
        alpha: Significance level
        
    Returns:
        Dictionary with test results
    """
    # Mann-Whitney U test
    stat, p_value = stats.mannwhitneyu(vals1, vals2, alternative='two-sided')
    
    # Effect size (r = Z / sqrt(N))
    n1, n2 = len(vals1), len(vals2)
    z_score = (stat - (n1 * n2 / 2)) / np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)
    effect_size = abs(z_score / np.sqrt(n1 + n2))
    
    return {
        'mann_whitney_p': p_value,
        'effect_size': effect_size,
        'mean_diff': np.mean(vals1) - np.mean(vals2),
        'significant': p_value < alpha
    }

def run_analysis(data: pd.DataFrame, metric: str, metric_type: str, n: int, alpha: float) -> List[Dict]:
    """
    Run appropriate statistical tests based on metric type.
    
    Args:
        data: DataFrame with metrics
        metric: Name of metric to analyze
        metric_type: 'proportion', 'continuous', or 'count'
        n: Sample size (for proportion tests)
        alpha: Significance level
    """
    results = []
    models = data.iloc[:, 0].unique()
    
    for i in range(len(models)):
        for j in range(i + 1, len(models)):
            model1_data = data[data.iloc[:, 0] == models[i]][metric]
            model2_data = data[data.iloc[:, 0] == models[j]][metric]
            
            if metric_type == 'proportion':
                stats_results = analyze_proportions(
                    float(model1_data.iloc[0]),
                    float(model2_data.iloc[0]),
                    n,
                    alpha
                )
            else:  # continuous or count
                stats_results = analyze_continuous(
                    model1_data.to_numpy(),
                    model2_data.to_numpy(),
                    alpha
                )
            
            results.append({
                'model1': models[i],
                'model2': models[j],
                'metric': metric,
                'metric_type': metric_type,
                **stats_results
            })
    
    return results

def write_results(results: List[Dict], output_path: str, show_all: bool = False, n_comparisons: int = 0):
    """
    Write analysis results to file.
    
    Args:
        results: List of test results
        output_path: Path to output file
        show_all: Whether to show all results or only significant ones
        n_comparisons: Total number of comparisons made
    """
    with open(output_path, 'w') as f:
        f.write("Statistical Analysis Results\n")
        f.write("==========================\n\n")
        
        if not results:
            f.write("No significant differences found in any metrics.\n")
            return
            
        # Filter results if not showing all
        if not show_all:
            results = [r for r in results if r['significant']]
            if not results:
                f.write("No significant differences found.\n")
                return
        
        # Sort by significance and metric name
        results.sort(key=lambda x: (
            0 if x['significant'] else 1,
            x['metric'],
            min(x.get('z_test_p', float('inf')),
                x.get('fisher_p', float('inf')),
                x.get('mann_whitney_p', float('inf')))
        ))
        
        # Write metric descriptions
        f.write("Metric Descriptions:\n")
        f.write("-----------------\n")
        for metric in set(r['metric'] for r in results):
            if metric in METRIC_DESCRIPTIONS:
                f.write(f"{metric}: {METRIC_DESCRIPTIONS[metric]}\n")
        f.write("\n")
        
        # Write results
        f.write(f"Analysis Results {'(All)' if show_all else '(Significant Only)'}:\n")
        f.write("-----------------\n")
        
        for result in results:
            f.write(f"\nMetric: {result['metric']} ({result['metric_type']})\n")
            f.write(f"Models Compared: {result['model1']} vs {result['model2']}\n")
            
            if result['metric_type'] == 'proportion':
                f.write(f"Z-test p-value: {result['z_test_p']:.4f}\n")
                f.write(f"Fisher's exact p-value: {result['fisher_p']:.4f}\n")
                f.write(f"Absolute difference: {result['absolute_diff']:.4f}\n")
                f.write(f"Relative improvement: {result['relative_diff']:.1f}%\n")
            else:
                f.write(f"Mann-Whitney U p-value: {result['mann_whitney_p']:.4f}\n")
                f.write(f"Mean difference: {result['mean_diff']:.4f}\n")
                
            f.write(f"Effect size: {result['effect_size']:.4f}\n")
            f.write(f"Significant: {'Yes' if result['significant'] else 'No'}\n")
            f.write("-" * 50 + "\n")
        
        # Write summary
        f.write(f"\nSummary:\n")
        f.write(f"- Total comparisons made: {n_comparisons}\n")
        f.write(f"- Significant differences found: {len([r for r in results if r['significant']])}\n")

def main():
    """Main function to run the statistical analysis."""
    parser = argparse.ArgumentParser(description="Statistical Analysis for Grid World Navigation Metrics")
    parser.add_argument("input_file", help="Path to input CSV or Excel file")
    parser.add_argument("config_file", help="Path to configuration JSON file")
    parser.add_argument("--output_file", help="Path to output file")
    parser.add_argument("--alpha", type=float, default=0.05, help="Significance level")
    parser.add_argument("--sample_size", type=int, default=100, help="Sample size for proportion tests")
    parser.add_argument("--all", action="store_true", help="Show all results, not just significant ones")
    
    args = parser.parse_args()
        
    setup_logging()
    logging.info("Starting statistical analysis...")
    
    try:
        # Load config and data
        logging.info(f"Loading configuration from {args.config_file}")
        with open(args.config_file) as f:
            config = json.load(f)
        
        logging.info(f"Loading data from {args.input_file}")
        data = pd.read_csv(args.input_file) if args.input_file.endswith('.csv') \
               else pd.read_excel(args.input_file)
        
        results = []
        n_comparisons = 0
        
        # Run tests for each metric type
        for metric_type, metrics in [
            ('proportion', config.get('proportion_metrics', [])),
            ('continuous', config.get('continuous_metrics', [])),
            ('count', config.get('count_metrics', []))
        ]:
            for metric in metrics:
                if metric in data.columns:
                    logging.info(f"Analyzing metric: {metric} ({metric_type})")
                    metric_results = run_analysis(
                        data, metric, metric_type, args.sample_size, args.alpha
                    )
                    results.extend(metric_results)
                    n_comparisons += len(data.iloc[:, 0].unique()) * (len(data.iloc[:, 0].unique()) - 1) // 2
                else:
                    logging.warning(f"Metric {metric} not found in data")
        
        # Write results
        output_path = args.output_file or str(Path(args.input_file).with_suffix('')) + '_results.txt'
        write_results(results, output_path, args.all, n_comparisons)
        
        # Print completion message
        sig_count = len([r for r in results if r['significant']])
        print(f"\nAnalysis complete!")
        print(f"- Processed {len(data.columns)-1} metrics")
        print(f"- Made {n_comparisons} comparisons")
        print(f"- Found {sig_count} significant differences")
        print(f"Results written to: {output_path}")
        
    except Exception as e:
        logging.error(f"Error during analysis: {str(e)}")
        raise

if __name__ == "__main__":
    main()
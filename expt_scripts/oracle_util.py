import collections
import copy
import functools
import json
import pprint
import os
import random
import re
import scipy.stats

#TODO: move to expt_scripts

# compare output oracle functions to either (1) output of a step in a
# PT prompt or (2) the same step executed without context in the
# debugger - Table 7 in the paper

import arg_util
import debugger
import logutil
import expt_util

from mocks2.boolean_expressions import solve_negation_proxy as solve_negation
from mocks2.dyck_languages import is_open_paren_proxy as is_open_paren
from mocks2.dyck_languages import update_stack_proxy as update_stack
from mocks2.dyck_languages import empty_stack as empty_stack
from mocks2.multistep_arithmetic_two import is_simple_expression_proxy as is_simple_expression
from mocks2.multistep_arithmetic_two import eval_expression_proxy as eval_expression
from mocks2.multistep_arithmetic_two import eval_simple_expression_proxy as eval_simple_expression
from mocks2.multistep_arithmetic_two import eval_variabilized_expression_proxy as eval_variabilized_expression
from mocks2.multistep_arithmetic_two import rewrite_expression_proxy as rewrite_expression
from mocks2.tracking_shuffled_objects_three_objects import simulate_swap_proxy as simulate_swap
from mocks2.tracking_shuffled_objects_three_objects import answer_question_proxy as answer_question
from mocks2.word_sorting import bucket_sort_proxy as bucket_sort
from mocks2.word_sorting import flatten_proxy as flatten
from mocks2.word_sorting import partition_words_proxy as partition_words
from mocks2.word_sorting import kth_letter_proxy as kth_letter
from mocks2.word_sorting import sort_keys_proxy as sort_keys

def eval_oracle(fname: str, args: str, bindings, state):
    # evaluate a function fname on arguments args using the oracle implementation
    try:
        # get output by calling oracle
        return eval(f'{fname}{args}', bindings, {}) 
    except Exception as ex:
        print(f'*** oracle error for {fname}{args}: {ex}')
        return f'OracleError: {repr(ex)}'
    
def extract_step_output(state, fname, trace_lines, first=True):
    # given a generated program trace in trace_lines, extract the
    # output for a step for the function fname.  The step chosen is
    # the first one in the trace if first==True, otherwise the last
    outputs = state.step_outputs(trace_lines, fn=fname)
    if outputs:
        return outputs[0].value if first else outputs[-1].value
    # back off the last line of trace
    if '```' in trace_lines[-1]:
        return trace_lines[-2]
    return trace_lines[-1]

def eval_modular(fname: str, args: str, bindings, state):
    # evaluate a function fname on arguments args using a a prompted
    # model, accessible via the debugger state 'state'.
    step_command = f'{fname}{args}'
    print('step_command', step_command)
    step_trace = state.gentrace(step_command)
    trace_lines = step_trace.split('\n')
    outp = extract_step_output(state, fname, trace_lines)
    print('=' * 10, 'step_trace', '=' * 10)
    print(step_trace)
    print('=' * 10, 'end', '=' * 10)
    print(f'step output: {outp=}')
    return outp

def compare_to_oracle(str_output, oracle_output, bindings):
    try:
        return oracle_output == eval(str_output, bindings, {})
    except SyntaxError as ex:
        print(f'*** syntax error for {repr(str_output)}: {ex}')
        return 'SyntaxError'
    except Exception as ex:
        print(f'*** unexpected error for {repr(str_output)}: {ex}')
        return 'SyntaxError'

def pairs_to_evaluate(io_pairs_by_fn):
    for i, (fname, io_pairs) in enumerate(io_pairs_by_fn.items()):
        print(f'step {fname} ({i+1} / {len(io_pairs_by_fn)}) has {len(io_pairs)} samples')
        for io_pair in io_pairs:
            yield (fname, io_pair)

def compute_stats(io_pairs_by_fn, bindings, state, eval_fn=None):
    """Compute some statistics based on comparing evaluations to the oracle.
    
    If eval_fn is None the just compare the io_pairs.outp to the oracle.
    """
    results = collections.defaultdict(list)
    correctness_ctrs = collections.defaultdict(collections.Counter)
    for fname, io_pair in pairs_to_evaluate(io_pairs_by_fn):
        if eval_fn:
            outp = eval_fn(fname, io_pair.inp, bindings, state) 
        else:
            outp = io_pair.outp
        oracle_outp = eval_oracle(fname, io_pair.inp, bindings, state) 
        correctness = compare_to_oracle(outp, oracle_outp, bindings)
        results[fname].append((correctness, outp, io_pair))
        correctness_ctrs[fname][correctness] += 1
    return correctness_ctrs, results

def collect_samples(examples, args, bindings):
    # collect all triples of function, input, LLM output
    # for functions with an oracle implementation
    all_io_pairs_by_fn = collections.defaultdict(list)
    for ex in examples:
        num_bad, num_calls, call_summary, io_pairs_by_fn = logutil.parse_output(
            ex['output'])
        for fname, io_pairs in io_pairs_by_fn.items():
            if fname not in bindings:
                continue
            if args.step_name and fname not in args.step_name:
                continue
            all_io_pairs_by_fn[fname].extend(io_pairs)

    # use at most max_step_sample examples of each step
    random.seed(args.seed)
    def downsample(xs): 
        if len(xs) < args.max_step_sample:
            return xs
        else:
            return random.choices(xs, k=args.max_step_sample)
    sampled_io_pairs_by_fn = {
        fname: downsample(all_io_pairs_by_fn[fname])
        for fname in all_io_pairs_by_fn
    }
    print('sampled pairs:')
    for fname, io_pairs in sampled_io_pairs_by_fn.items():
        print(' . ', fname, len(io_pairs))
    return sampled_io_pairs_by_fn

def save_stats(task, tag, correctness_ctrs, results):
    json_dict = dict(
        correctness=correctness_ctrs,
        results=results)
    filename = os.path.join(args.output_dir, f'{task}-{tag}.json')
    with open(filename, 'w') as fp:
        s = json.dumps(json_dict, indent='  ')
        fp.write(s + '\n')
        print(f'wrote {task}-{tag}.json to {filename}')
    

# The oracle_util main is usually run like this:
#
# python3 oracle_util.py TASK --test_set --max_step_sample K [--check_modular_outputs]
#
# It will
# - load an existing log produced by 'run_eval.py TASK --test_set'
# - sample up to K inputs/outputs of each step that can be evaluated by
#   an oracle.
# - evaluate all the input/output pairs in the sample against the oracle
#   and record how many pairs were correct/incorrect for each oracle-implemented
#   step
# - if --check_modular_outputs is set, set up a debugger.State instance which
#   can be used to execute single-step prompts for the oracle steps, and
#   evaluate the same sample using the prompt, instead of the oracle
#
# results are saved in json files, eg in doctest-prompting-data/special-logs/oracle-logs
# and can be turned into dataframes/latex with the helper script
#   py analysis_scripts/oracle_report.py

if __name__ == "__main__":
    parser = arg_util.baseparser()
    # for sampling a subset of steps to evaluate
    parser.add_argument(
        '--max_step_sample',
        type=int,
        default=20,
        help='max number of samples of each step to draw'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=137,
        help='seed for random sampling'
    )
    parser.add_argument(
        '--output_dir',
        default='../doctest-prompting-data/special-logs/oracle-logs',
        help='where to save info'
    )
    parser.add_argument(
        '--step_name',
        action='append',
        help='steps to process'
    )
    arg_util.add_logging_options(parser)
    # for finishing a program trace given a prefix
    parser.add_argument(
        '--check_modular_outputs',
        action='store_true',
        help='compare oracle accuracy running individual steps')
    args = parser.parse_args()
    if args.test_set:
        args.example_dir = '../doctest-prompting-data/data/test/'

    # load examples
    filename = arg_util.log_file(args)
    print(f'Loading log file: {filename}')
    examples = logutil.load_examples(filename, verbose=True)

    # sample a subset of oracle-evaluable steps
    sampled_io_pairs_by_fn = collect_samples(examples, args, locals())

    # a way to evaluate using an LLM
    state = debugger.State.from_args(copy.copy(args)) if args.check_modular_outputs else None

    # evaluate everything with oracles
    ptp_correctness_ctrs, ptp_results = compute_stats(
        sampled_io_pairs_by_fn, locals(), state, None)

    for fname in ptp_correctness_ctrs:
        print(fname, ptp_correctness_ctrs[fname])
    save_stats(args.task, 'ptp', ptp_correctness_ctrs, ptp_results)

    if args.check_modular_outputs:
        modular_correctness_ctrs, modular_results = compute_stats(
            sampled_io_pairs_by_fn, locals(), state, eval_modular)
        save_stats(args.task, 'mod', modular_correctness_ctrs, modular_results)
        for fname in modular_correctness_ctrs:
            print(f'results for step {fname}:')
            print(' . ptp correctness', ptp_correctness_ctrs[fname])
            print(' . mod correctness', modular_correctness_ctrs[fname])
            print(' . result comparison:')
            compare_ctr = collections.Counter()
            for ptp_res, mod_res in zip(ptp_results[fname], modular_results[fname]):
                ptp_corr, ptp_outp, ptp_iop = ptp_res
                mod_corr, mod_outp, mod_iop = mod_res
                if ptp_iop != mod_iop:
                    print('out of sync!', ptp_res, mod_res)
                compare_ctr['total'] += 1
                if ptp_corr != mod_corr:
                    print(f' . . difference {ptp_corr=} {mod_corr=} {ptp_outp=} {mod_outp=} {ptp_iop=}')
                    compare_ctr['different'] += 1
                    compare_ctr['ptp_win'] += 1 if ptp_corr else 0
            print(' . .', compare_ctr)

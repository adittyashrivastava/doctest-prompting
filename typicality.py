import math
import pandas as pd
import nltk

from collections import Counter
from dataclasses import dataclass, field
from nltk.lm.preprocessing import everygrams
from nltk.lm.preprocessing import flatten
from nltk.lm.preprocessing import pad_both_ends
from scipy.stats import binomtest
from scipy.stats import fisher_exact

import audit

@dataclass
class TypicalityModel:
    results: list[audit.Auditor]
    scores: list[float] = field(default_factory=list)
    df: pd.DataFrame = field(default_factory=pd.DataFrame)

    def fit(self):
        """Fit a model to the results.
        """
        ...

    def predict_score(self, trace_lines: list[str]) -> float:
        """Predict the score for a trace.
        
        trace_lines should be the text between <program_trace>
        and </program_trace> split by newlines.
        """
        ...

    def is_score_above_median(self, trace_lines: list[str]) -> bool:
        """A boolean score
        """
        predicted_score = self.predict_score(trace_lines)
        sorted_scores = sorted(self.scores)
        mid = len(sorted_scores) // 2
        return predicted_score > sorted_scores[mid]

    def postfit(self):
        """Call this before generating reports.
        """
        self.df = pd.DataFrame(
            dict(
                correctness = [int(r.is_correct) for r in self.results],
                index = range(len(self.results)),
                score = self.scores))
        self.df = self.df.sort_values('score')
        return self

    def report(self, num_splits=2, fisher_exact_test=True, verbose=1):
        """Print out and then return some stats.
        """

        def get_n_k(i):
            w = len(self.df) // num_splits
            ranges = [(w * i, w * (i+1)) for i in range(num_splits)]
            lo, hi = ranges[i]
            n = hi - lo
            k = self.df.iloc[lo:hi].correctness.sum()
            return n, k

        tot_n = len(self.df)
        tot_k = self.df.correctness.sum()

        rows = []
        acc = tot_k / tot_n
        for i in range(num_splits):
            n, k = get_n_k(i)
            if fisher_exact_test:
                pvalue = fisher_exact([[tot_k, tot_n], [k, n]]).pvalue
            else:
                pvalue = binomtest(n=n, k=k, p=acc).pvalue
            rows.append(dict(n=n, k=k, acc = k/n, pvalue=pvalue))
        interval_df = pd.DataFrame(rows)
            
        rows = []
        for i in range(num_splits - 1):
            n1, k1 = get_n_k(i)
            acc1 = k1 / n1
            for j in range(i+1, num_splits):
                n2, k2 = get_n_k(j)
                acc2 = k2 / n2
                pvalue = fisher_exact([[n1,k1],[n2,k2]]).pvalue
                rows.append(dict(
                    i=i, j=j, 
                    k1=k1, n1=n1, acc1=acc1, 
                    k2=k2, n2=n2, acc2=acc2, pvalue=pvalue))
        pairings_df = pd.DataFrame(rows)

        if verbose:
            print()
            print(f'typicality audit {self.__class__.__name__} accuracy by segment')
            print(interval_df)
            print()
            print(f'differences between segments')
            print(pairings_df)
        
        return interval_df, pairings_df
            
@dataclass
class SkeletonLogProb(TypicalityModel):
    """Score a trace by the log probility of its 'skeleton',
    ie the sequence of step functions.
    
    Example usage:
    >>> model = typicality.SkeletonLogProb(runner.results).fit()
    >>> model.postfit().report()
    >>> d = model.as_dict()
    >>> model2 = SkeletonLogProb.from_dict(d)
    """

    def fit(self):
        self.skeletons = [self.skeleton(r) for r in self.results]
        self.freq = Counter(self.skeletons)
        self.tot = sum(self.freq.values())
        self.scores = [ 
            self.logprob(self.skeletons[i]) 
            for i in range(len(self.skeletons))]
        return self

    def logprob(self, sk):
        # smooth toward prior of 1/self.freq
        num = self.freq.get(sk, 0.0) + 1.0/len(self.freq)
        denom =  self.tot + 1.0
        return math.log(num/denom)

    def predict_score(self, trace_lines):
        trace = audit.Trace(
            trace_lines,
            do_eval=False,
            do_combine_multiline_steps=False,
            do_expand_compressed_trace=False)
        r = audit.NullAuditor(trace)
        skeleton = self.skeleton(r)
        return self.logprob(skeleton)

    def skeleton(self, result):
        return ' '.join(result.df.step_fn)

    def as_dict(self):
        """For quick and compact serialization.

        Serializing the dict with json and deserializing it will let
        you use model.predict_score and model.is_score_above_median
        but not much else.
        """
        return dict(
            results=[],
            scores=self.scores,
            freq=dict(self.freq),
            tot=self.tot)

    @staticmethod
    def from_dict(d):
        """For quick and compact deserialization.
        """
        model = SkeletonLogProb([])
        model.scores = d['scores']
        model.freq = d['freq']
        model.tot = d['tot']
        return model

@dataclass
class Errors(TypicalityModel):
    
    def fit(self):
        self.scores = [-len(r.trace.errors) for r in self.results]
        return self

@dataclass
class Audits(TypicalityModel):
    
    def fit(self):
        self.scores = [
            (len([a for a in r.audits if a['passed']]) - 
             len([a for a in r.audits if not a['passed']]))
            for r in self.results]
        return self

@dataclass
class AuditsPassed(TypicalityModel):
    
    def fit(self):
        self.scores = [
            len([a for a in r.audits if a['passed']])
            for r in self.results]
        return self

@dataclass
class Ngrams(TypicalityModel):
    
    def fit(self, max_len=2):
        self.docs = [
            everygrams(pad_both_ends(r.df.step_fn, 2), 2)
            for r in self.results]
        self.train = self.docs
        self.ctr = Counter([ng for doc in self.docs for ng in doc])
        self.vocab = nltk.lm.Vocabulary(self.ctr, unk_cutoff = 1, unk_label=("<UNK>",))
        self.lm = nltk.lm.MLE(2)
        self.lm.fit(self.train, self.vocab)
        self.scores = [self.lm.perplexity(doc) for doc in self.docs]

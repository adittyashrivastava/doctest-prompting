import argparse
import json
import os
import re
from scipy.stats import fisher_exact
import sys

import typicality


HMM_SWEEP_RESULTS = dict(
    medqa=(1, 25),
    mmlu_pro_biology=(2, 50),
    mmlu_pro_health=(2, 50),
    pubmedqa=(2, 25),
    medcalc_bench_formulas=(2, 25),
    medcalc_bench_rules=(2, 50),
)

def print_symbolic_df_as_latex(df, stem, n):
    fdf = df[df.stem == stem][df.n1 > 2][df.n2 > 2] .copy()
    print('stem', stem, 'mean diff', fdf['diff'].mean())
    fdf['pct fail'] = [100 * x / n for x in fdf.n1]
    fields = ['pct fail']
    fdf['failing'] = [x * 100 for x in fdf.acc1]
    fdf['passing'] = [x * 100 for x in fdf.acc2]
    fdf['delta'] = [x * 100 for x in fdf['diff']]
    fields.extend(['failing', 'passing', 'delta', 'pvalue'])
    fields.append('audit')        
    fdf = fdf[fields]
    fdf = fdf.sort_values(by='pct fail')
    opts = dict(
        float_format='%.1f',
        index=True)
    print(fdf)
    print(fdf.to_latex(**opts))
    return fdf

def collect_symbolic_audit_results(args, stem, runner):

    def fails(r, msg):
        return [a for a in r.audits if a['msg'] == msg and not a['passed']]

    def create_row(stem, msg, failing_results, passing_results):
        def get_n_k(results):
            n = len(results)
            k = len([r for r in results if r.is_correct])
            return n, k
        n1, k1 = get_n_k(failing_results)
        n2, k2 = get_n_k(passing_results)
        if n1 == 0 or n2 == 0:
            return None
        acc1 = k1 / n1
        acc2 = k2 / n2
        diff = acc2 - acc1
        pvalue = fisher_exact([[n1, k1], [n2, k2]]).pvalue
        return dict(
            stem=stem, audit=msg, 
            k1=k1, n1=n1, acc1=acc1, 
            k2=k2, n2=n2, acc2=acc2, 
            diff=(acc2 - acc1), pvalue=pvalue)

    rows = []
    all_audit_names = set(a['msg'] for r in runner.results for a in r.audits)
    for msg in all_audit_names:
        failing_results = [r for r in runner.results if fails(r, msg)]
        passing_results = [r for r in runner.results if not fails(r, msg)]
        row = create_row(stem, msg, failing_results, passing_results)
        if row is not None:
            rows.append(row)
    return rows

def collect_typicality_results(
        args, stem, runner,
        error_results=False,
        multinomial_results=True,
        bigram_results=True,
        trigram_results=True,
        old_skeleton_results=False,
        skeleton_results=True,
        hmm_results=True):
        

    def stats(method, model_dfs):
        segment_df, diff_df, tau = model_dfs
        acc1 = diff_df.acc1.iloc[0]
        acc2 = diff_df.acc2.iloc[0]
        diff = acc2 - acc1
        pvalue = diff_df.pvalue.iloc[0]
        return dict(
            stem=stem, method=method, tau=tau,
            acc1=acc1, acc2=acc2, diff=diff, pvalue=pvalue)

    all_stats = []
    if error_results:
        print('\n ---- by errors ----')
        model = typicality.Errors(runner.results)
        errors_dfs = model.fit().postfit().report(num_splits=2)
        all_stats.append(stats('errors', errors_dfs))

    if multinomial_results:
        print('\n ---- by multinomial ----')
        model = typicality.Hmm(runner.results)
        multinomial_dfs = model.fit(n_components=1, ngram_size=0).postfit().report(num_splits=2)
        all_stats.append(stats('multinomial', multinomial_dfs))

    if bigram_results:
        print('\n ---- by bigram ----')
        model = typicality.Hmm(runner.results)
        bigram_dfs = model.fit(n_components=1, ngram_size=2).postfit().report(num_splits=2)
        all_stats.append(stats('bigram', bigram_dfs))
        
    if trigram_results:
        print('\n ---- by trigram ----')
        model = typicality.Hmm(runner.results)
        trigram_dfs = model.fit(n_components=1, ngram_size=3).postfit().report(num_splits=2)
        all_stats.append(stats('trigram', trigram_dfs))


    if old_skeleton_results:
        print('\n ---- by skeleton prob with old package ----')
        model = typicality.SkeletonLogProb(runner.results)
        skeleton_dfs = model.fit().postfit().report(num_splits=2)
        all_stats.append(stats('skeleton-old', skeleton_dfs))

        if args.skeleton_save_file:
            print(f'saving skeleton params to {args.skeleton_save_file}')
            with open(args.skeleton_save_file, 'w') as fp:
                json.dump(model.as_dict(), fp, indent=2)

    if skeleton_results:
        print('\n ---- by skeleton prob with hmms ----')
        model = typicality.Hmm(runner.results)
        skeleton_dfs = model.fit(n_components=1, ngram_size=9999).postfit().report(num_splits=2)
        all_stats.append(stats('skeleton', skeleton_dfs))

    if hmm_results:
        print('\n ---- with hmms ----')
        model = typicality.Hmm(runner.results)
        # precomputed sweep parameters if available
        n_components, ngram_size = HMM_SWEEP_RESULTS.get(stem, (None, None))
        if n_components is not None:
            model.fit(n_components=n_components, ngram_size=ngram_size)
        else:
            model.sweep_and_fit(
                n_components_choices=(2, 5, 10),
                ngram_size_choices=(0, 3, 10, 25, 50))
        hmm_dfs = model.postfit().report(num_splits=2)
        all_stats.append(stats('hmm*', hmm_dfs))
    return all_stats

def load_json_object(filename):
    """Load a json object from a file.
    """
    with open(filename, 'r') as fp:
        json_obj = json.load(fp)
    return json_obj

def get_tag(tag, pred):
    """Find <tag>text inside a tag</tag>.
    """
    start = f'<{tag}>\n'
    end = f'\n</{tag}>'
    return pred[pred.find(start) + len(start):pred.find(end)]

def get_answer(boxed_answer):
    """Extract an answer that was boxed{like this}.
    """
    return re.sub(r'.*boxed\{(.*)\}.*',r'\1', boxed_answer)    

def load_json_details(filename):
    """Load a filename in Jixuan's json format.

    Output is list of dicts with fields: prediction, target,
    trace, boxed_answer, answer, trace_lines.
    """
    json_obj = load_json_object(filename)
    data = [
        dict(
            input=json_obj[i]['example'][0], 
            specifics=json_obj[i]['specifics'],
            prediction=json_obj[i]['predictions'][0],
            target=json_obj[i]['target'][0])
        for i in range(len(json_obj))]
    padding = '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'
    data = [d for d in data if padding not in d['prediction']]
    for d in data:
        p = d['prediction']
        trace_lines = get_tag('program_trace', p)
        boxed_answer=get_tag('answer', p)
        answer = get_answer(boxed_answer)
        d.update(
            output=trace_lines.split('\n'),
            boxed_answer=boxed_answer,
            answer=answer)
    return data

def load_json_correctness(filename, key1=None, key2=None):
    json_obj = load_json_object(filename)
    metric_values = json_obj['metrics_values']
    def follow_key(d, d_name, key):
        if key is None:
            keys = list(d.keys())
            if len(keys) != 1:
                raise ValueError(f'multiple {d_name} keys: {keys}')
            key = keys[0]
        try:
            val = d[key]
        except KeyError as ex:
            raise ValueError(f'{d_name} missing key {key}: {d.keys()}')
        return val
    metrics = follow_key(metric_values, 'metrics_values', key1)
    correctness = follow_key(metrics, 'metrics', key2)
    return correctness

def audit_argparser():
    parser = argparse.ArgumentParser(prog=sys.argv[0].replace(".py", ""))
    parser.add_argument(
        '--file',
        default='../../doctest-prompting-data/jl-json/custom_medcalc_bench_0_0.0_0_0_0.json',
        help='details json log to load')
    parser.add_argument(
        '--auditor_class_name',
    )
    parser.add_argument(
        '--partial_program_file',
        help='partial program file'
    )
    return parser

def get_auditor_class(module, args):
    """Figure out the auditor class given a string name.
    """
    auditor_class = None
    if args.auditor_class_name:
        try:
            auditor_class = getattr(module, args.auditor_class_name)
        except AttributeError as ex:
            print('using default auditor_class:', ex)
    return auditor_class

def load_partial_program_defs(partial_program_file):
    """Ensure you can 'eval' things defined in a partial_program_file.
    """
    if partial_program_file is None:
        print('no partial_program_file set')
    else:
        print('loading definitions from', partial_program_file)
        os.makedirs('tmp_stubs', exist_ok=True)
        with open('tmp_stubs/stubs.py', 'w') as fp:
            for line in open(partial_program_file):
                if not line.startswith('@traced'):
                    fp.write(line)

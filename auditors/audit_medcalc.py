import collections
import json
import sys

import log_util
import audit
import auditors.util as util

#
# auditors for strong prompted methods
#

class AuditMedcalcFormula(audit.Auditor):

    def test_analyze_input(self):
        df = self.df
        self.assertTrue(
            msg=f'one "analyze_input" step',
            expr=(len(df[df.step_fn == 'analyze_input']) == 1))

    def test_get_data(self):
        df = self.df
        self.assertTrue(
            msg=f'one "get_data" step',
            expr=(len(df[df.step_fn == 'get_data']) == 1))
        
    def test_convert_data(self):
        df = self.df
        datapoints = df[df.step_fn == 'get_data'].output.iloc[0]
        self.assertTrue(
            msg="all datapoints converted",
            expr=(len(datapoints) == len(df[df.step_fn == 'convert_units'])))

class AuditMedcalcRules(audit.Auditor):

    def test_analyze_input(self):
        df = self.df
        self.assertTrue(
            msg=f'one step with step_fn of "analyze_input"',
            expr=(len(df[df.step_fn == 'analyze_input']) == 1))
        
    def test_each_rule_applied(self):
        df = self.df
        analysis_parts = df.query('step_fn == "analyze_input"').output.iloc[0]
        try:
            n_parts = len(analysis_parts)
        except TypeError:
            n_parts = -1
        self.assertTrue(
            msg=f'analyze_input returns three values',
            expr=(n_parts == 3))
        if n_parts == 3:
            _, rules, _ = analysis_parts
            num_rules = len(rules)
            for step_fn in ['get_data', 'convert_units', 'check_rule', 'accumulate_score']:
                self.assertTrue(
                    msg=f'one step per rule with step_fn of "{step_fn}"',
                    expr=(len(df[df.step_fn == step_fn]) == num_rules))
            

class AuditMedcalcRulesNoteless(audit.Auditor):

    def test_analyze_input(self):
        df = self.df
        self.assertTrue(
            msg=f'one step with step_fn of "analyze_input"',
            expr=(len(df[df.step_fn == 'analyze_input']) == 1))
        
    def test_each_rule_applied(self):
        df = self.df
        analysis_parts = df.query('step_fn == "analyze_input"').output.iloc[0]
        self.assertTrue(
            msg=f'analyze_input returns two values',
            expr=(len(analysis_parts) == 2))
        if len(analysis_parts) == 2:
            _, rules = analysis_parts
            num_rules = len(rules)
            for step_fn in ['get_data', 'convert_units', 'check_rule', 'accumulate_score']:
                self.assertTrue(
                    msg=f'one step per rule with step_fn of "{step_fn}"',
                    expr=(len(df[df.step_fn == step_fn]) == num_rules))


#
# looser auditor for small models - which doesn't really seem to work
#


class AuditMedcalcRulesNotelessLoose(audit.Auditor):

    def test_analyze_input(self):
        df = self.df
        self.assertTrue(
            msg=f'one step with step_fn of "analyze_input"',
            expr=(len(df[df.step_fn == 'analyze_input']) == 1))
        
    def test_rules_by_list(self):
        df = self.df
        analysis_parts = df.query('step_fn == "analyze_input"').output.iloc[0]
        self.assertTrue(
            msg=f'analyze_input returns two values',
            expr=(len(analysis_parts) == 2))
        if len(analysis_parts) == 2:
            _, rules = analysis_parts
            num_rules = len(rules)
            for step_fn in ['get_data', 'convert_units', 'check_rule', 'accumulate_score']:
                self.assertTrue(
                    msg=f'one step per rule with step_fn of "{step_fn}"',
                    expr=(len(df[df.step_fn == step_fn]) == num_rules))


    def test_each_rule_fully_applied(self):
        df = self.df
        num_steps = {
            step_fn: len(df[df.step_fn == step_fn])
            for step_fn in [
                    'get_data', 'convert_units', 
                    'check_rule', 'accumulate_score']
        }
        self.assertTrue(
            msg=f'convert_units step for every get_data step',
            expr=(num_steps['convert_units'] == num_steps['get_data']))
        self.assertTrue(
            msg=f'evaluate_rule step for every get_data step',
            expr=(num_steps['check_rule'] == num_steps['get_data']))
        self.assertTrue(
            msg=f'accumulate_score step for every get_data step',
            expr=(num_steps['accumulate_score'] == num_steps['get_data']))

#
# a loose auditor for small SFT/RL models
#

class AuditMedcalcRulesLooseJson(audit.Auditor):

    def test_analyze_input(self):
        df = self.df
        self.assertTrue(
            msg=f'one step with step_fn of "analyze_input"',
            expr=(len(df[df.step_fn == 'analyze_input']) == 1))
        
    def test_each_rule_fully_applied(self):
        # mock uses 'check_rule' instead of 'evaluate_rule' - wtf?
        df = self.df
        num_steps = {
            step_fn: len(df[df.step_fn == step_fn])
            for step_fn in ['get_data', 'convert_units', 'evaluate_rule', 'accumulate_score']
        }
        self.assertTrue(
            msg=f'convert_units step for every get_data step',
            expr=(num_steps['convert_units'] == num_steps['get_data']))
        self.assertTrue(
            msg=f'evaluate_rule step for every get_data step',
            expr=(num_steps['evaluate_rule'] == num_steps['get_data']))
        self.assertTrue(
            msg=f'accumulate_score step for every get_data step',
            expr=(num_steps['accumulate_score'] == num_steps['get_data']))


#
# utils for medcalc data
#

import medcalc.utils.medcalc_rules
import medcalc.utils.medcalc_formulas

# reference: https://github.com/ncbi-nlp/MedCalc-Bench/blob/main/evaluation/evaluate.py
# via jixuan PTP_R1/src/open_r1/evaluate.py

def medicalc_bench_check(predictions: list[str], formatted_doc: Doc, **kwargs) -> float:
    calid = formatted_doc.specific['calid']
    # output_type = formatted_doc.specific['output_type']
    lower_limit = formatted_doc.specific['lower_limit']
    upper_limit = formatted_doc.specific['upper_limit']
    answer = predictions[0]
    ground_truth = formatted_doc.choices[formatted_doc.gold_index]
    calid = int(calid)
    # extract the answer, The answer is: $\\boxed{{ANSWER}}$
    match = re.search(r"\$?\\boxed{(.*?)}\$?", answer)
    if match:
        answer = match.group(1).strip()
    else:
        answer = answer.strip()

    try:
        if calid in [13, 68]:
            if datetime.strptime(answer, "%m/%d/%Y").strftime("%-m/%-d/%Y") == datetime.strptime(ground_truth, "%m/%d/%Y").strftime("%-m/%-d/%Y"):
                correctness = 1
            else:
                correctness = 0
        elif calid in [69]:
            # Output Type: integer (A, B)
            match = re.search(r"\(?[\"\']?(\d+)\s*(weeks?)?[\"\']?,?\s*[\"\']?(\d+)\s*(days?)?[\"\']?\s*\)?", ground_truth)
            ground_truth = f"({match.group(1)}, {match.group(3)})"
            match = re.search(r"\(?[\"\']?(\d+)\s*(weeks?)?[\"\']?,?\s*[\"\']?(\d+)\s*(days?)?[\"\']?\s*\)?", answer)
            if match:
                weeks = match.group(1)
                days = match.group(3)
                answer = f"({weeks}, {days})"
                if eval(answer) == eval(ground_truth):
                    correctness = 1
                else:
                    correctness = 0
            else:
                correctness = 0
        elif calid in [4, 15, 16, 17, 18, 20, 21, 25, 27, 28, 29, 32, 33, 36, 43, 45, 48, 51, 69]:
            # Output Type: integer A
            answer = round(eval(answer))
            if answer == eval(ground_truth):
                correctness = 1
            else:
                correctness = 0
        elif calid in [2,  3,  5,  6,  7,  8,  9, 10, 11, 19, 22, 23, 24, 26, 30, 31, 38, 39, 40, 44, 46, 49, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]:
            answer = eval(answer)
            if answer >= eval(lower_limit) and answer <= eval(upper_limit):
                correctness = 1
            else:
                correctness = 0
        else:
            raise ValueError(f"Unknown calculator ID: {calid}")
    except Exception as e:
        print(f"Error in calculator ID {calid}: {e}, answer: {answer}, ground_truth: {ground_truth}")
        correctness = 0
    return correctness


def is_correct(answer, specifics, target):
    try:
        a = float(answer)
        lo = float(specifics['lower_limit'])
        hi = float(specifics['upper_limit'])
        return lo <= a <= hi
    except ValueError:
        None
    return False

def which_kind(d):
    rule_calc_names = medcalc.utils.medcalc_rules.rules.keys()
    formula_calc_names = medcalc.utils.medcalc_formulas.formulas.keys()
    trace = ''.join(d['output']).lower()
    for name in rule_calc_names:
        if name.lower() in trace:
            return 'rule'
    for name in formula_calc_names:
        if name.lower() in trace:
            return 'formula'
    return None

if __name__ == '__main__':
    parser = util.audit_argparser()
    parser.add_argument(
        '--calc_type',
        default='any',
        choices=['rule', 'formula', 'any'])
    parser.add_argument(
        '--og_example_file',
        default='../medcalc/examples/test/medcalc_formulas.json',
        help='original example file with inputs, targets, and ranges')
    args = parser.parse_args()
    print(args)
    auditor_class = util.get_auditor_class(sys.modules[__name__], args)

    if args.file.endswith('.json'):
        data = util.load_json_details(args.file)
        for d in data:
            d['is_correct'] = correctness = is_correct(d['answer'], d['specifics'], d['target'])
        if (args.calc_type != 'any'):
            print(f'before filtering by {args.calc_type}', len(data))
            data = [d for d in data if which_kind(d) == args.calc_type]
            print(f'after filtering by {args.calc_type}', len(data))

        if auditor_class is None:
            # reasonable default
            auditor_class = dict(
                rule=AuditMedcalcRulesLooseJson,
                formula=audit.NullAuditor,
                any=audit.NullAuditor)[args.calc_type]

        print('auditor_class:', auditor_class)
        runner = audit.Runner(data, auditor_class, do_eval=False)
        
    elif args.file.endswith('.log'):
        if args.calc_type == 'rule' and auditor_class is None:
            # reasonable default
            if '_noteless' in args.file:
                auditor_class = AuditMedcalcRulesNoteless
            else:
                auditor_class = AuditMedcalcRules
        elif args.calc_type == 'formula' and auditor_class is None:
            auditor_class = AuditMedcalcFormula
        else:
            # must be 'any'
            auditor_class = audit.NullAuditor

        if args.calc_type != 'formula':
            print('rule auditor_class:', auditor_class)
            runner = audit.Runner.from_logfile(args.file, auditor_class)
        else:
            log_examples = log_util.load_examples(args.file)
            print(f'loaded {len(log_examples)} examples from {args.file}')
            og_example_file = args.og_example_file
            with open(og_example_file) as fp:
                og_examples = json.loads(fp.read())['examples']
            print(f'loaded {len(log_examples)} from log and {len(og_examples)} with ranges from {args.og_example_file}')
            flips = collections.Counter()
            for ex, og_ex in zip(log_examples, og_examples):
                if 'range' in og_ex:
                    try:
                        target_min = float(og_ex['range']['minimum'])
                        target_max = float(og_ex['range']['maximum'])
                        prediction_num = float(ex['y_hat'])
                        is_correct = prediction_num >= target_min and prediction_num <= target_max
                        flips[(ex["is_correct"], is_correct)] += 1
                        ex["is_correct"] = is_correct
                    except KeyError:
                        print(ex)
                        raise KeyError()
                    except ValueError:
                        pass
            print(f'range correction: {flips}')
            print('auditor_class:', auditor_class)
            runner = audit.Runner(log_examples, auditor_class)

    else:
        raise NotImplementedError(args.format)

    runner.run()
    dfs = runner.report(verbose=1)

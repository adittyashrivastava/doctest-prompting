import argparse
import json
import pandas as pd
import os
import re
import sys

import audit
import auditors.util as util
import log_util
import typicality

METRIC = dict(
    aime24='ptp_math_pass@1:32_samples',
    math_500='ptp_extractive_match',
    gpqa_diamond='ptp_extractive_match',
    gsm8k='ptp_extractive_match',
    )

HMM_SWEEP_RESULTS = dict(
    medqa=(1, 25),
    mmlu_pro_biology=(2, 50),
    mmlu_pro_health=(2, 50),
    pubmedqa=(2, 25),
    medcalc_bench_formulas=(2, 25),
    medcalc_bench_rules=(2, 50),
)

def process(basedir, stem, cached_result, metric):

    filename = f'custom_{stem}_0_0.0_0_0_0.json'
    data = util.load_json_details(
        os.path.join(basedir, 'details', filename))
    if cached_result:
        results = util.load_json_correctness(
            os.path.join(basedir, 'results', filename),
            key2=metric)
        print('using cached result file')
        for d, y in zip(data, results):
            d['is_correct'] = bool(y)
    else:
        for d in data:
            d['is_correct'] = d['answer'] == d['target']

    auditor_class = audit.NullAuditor
    print('auditor_class:', auditor_class)
    runner = audit.Runner(data, auditor_class, do_eval=True)
    runner.run()

    def stats(method, model_dfs):
        segment_df, diff_df = model_dfs
        acc1 = diff_df.acc1.iloc[0]
        acc2 = diff_df.acc2.iloc[0]
        diff = acc2 - acc1
        pvalue = diff_df.pvalue.iloc[0]
        return dict(
            stem=stem, method=method,
            acc1=acc1, acc2=acc2, diff=diff, pvalue=pvalue)

    all_stats = []
    print('\n ---- by errors ----')
    model = typicality.Errors(runner.results)
    errors_dfs = model.fit().postfit().report(num_splits=2)
    all_stats.append(stats('errors', errors_dfs))

    print('\n ---- by multinomial ----')
    model = typicality.Hmm(runner.results)
    multinomial_dfs = model.fit(n_components=1, ngram_size=0).postfit().report(num_splits=2)
    all_stats.append(stats('multinomial', multinomial_dfs))

    print('\n ---- by skeleton prob ----')
    model = typicality.SkeletonLogProb(runner.results)
    skeleton_dfs = model.fit().postfit().report(num_splits=2)
    all_stats.append(stats('skeleton', skeleton_dfs))

    print('\n ---- by skeleton prob ----')
    model = typicality.Hmm(runner.results)
    skeleton_dfs = model.fit(n_components=1, ngram_size=9999).postfit().report(num_splits=2)
    all_stats.append(stats('skeleton1', skeleton_dfs))

    print('\n ---- with hmms ----')
    model = typicality.Hmm(runner.results)
    # precomputed sweep parameters if available
    n_components, ngram_size = HMM_SWEEP_RESULTS.get(args.stem, (None, None))
    if n_components is not None:
        model.fit(n_components=n_components, ngram_size=ngram_size)
    else:
        model.sweep_and_fit(
            n_components_choices=(2, 5, 10),
            ngram_size_choices=(0, 3, 10, 25, 50))
    hmm_dfs = model.postfit().report(num_splits=2)
    all_stats.append(stats('hmm*', hmm_dfs))
    return all_stats


if __name__ == '__main__':
    parser = util.audit_argparser()
    parser.add_argument(
        '--basedir',
        default='/Users/wcohen/Documents/code/PTP_R1/evals/Qwen2.5-7B-Base-RL-Clean-V2/',
        help='dir to load from')
    parser.add_argument(
	'--stem',
	required=True,
        default='medqa',
	help='filestem, eg medcalc_bench_rules')
    parser.add_argument(
	'--cached_result',
        action='store_true',
	help='access stored results')
    parser.add_argument(
        '--metric',
        help='metric in cached_result'
    )

    args = parser.parse_args()

    rows = []
    for stem in ['gpqa_diamond', 'medqa', 'mmlu_pro_biology',
                 'mmlu_pro_health', 'pubmedqa']:
        rows.extend(process(
            args.basedir, stem, args.cached_result, 
            (args.metric or METRIC.get(args.stem))))
    df = pd.DataFrame(rows)
    print()
    print(df)

import argparse
import json
import pandas as pd
import re
import sys

import audit
import medcalc.utils.medcalc_rules
import medcalc.utils.medcalc_formulas

class AuditMedcalcRules(audit.Auditer):

    def test_analyze_input(self):
        df = self.df
        self.assertTrue(
            msg=f'one step with step_fn of "analyze_input"',
            expr=(len(df[df.step_fn == 'analyze_input']) == 1))
        
    def test_each_rule_fully_applied(self):
        # mock uses 'check_rule' instead of 'evaluate_rule' - wtf?
        df = self.df
        num_steps = {
            step_fn: len(df[df.step_fn == step_fn])
            for step_fn in ['get_data', 'convert_units', 'evaluate_rule', 'accumulate_score']
        }
        self.assertTrue(
            msg=f'convert_units step for every get_data step',
            expr=(num_steps['convert_units'] == num_steps['get_data']))
        self.assertTrue(
            msg=f'evaluate_rule step for every get_data step',
            expr=(num_steps['evaluate_rule'] == num_steps['get_data']))
        self.assertTrue(
            msg=f'accumulate_score step for every get_data step',
            expr=(num_steps['accumulate_score'] == num_steps['get_data']))


def load_json_object(filename):
    with open(filename, 'r') as fp:
        json_obj = json.load(fp)
    return json_obj

def get_tag(tag, pred):
    start = f'<{tag}>\n'
    end = f'\n</{tag}>'
    return pred[pred.find(start) + len(start):pred.find(end)]

def get_answer(boxed_answer):
    return re.sub(r'.*boxed\{(.*)\}.*',r'\1', boxed_answer)    

def load_json_details(filename):
    """
    Output is list of dicts with fields: prediction, target,
    trace, boxed_answer, answer, is_correct, trace_lines, skeleton
    """
    json_obj = load_json_object(filename)
    data = [
        dict(
            input=json_obj[i]['example'][0], 
            specifics=json_obj[i]['specifics'],
            prediction=json_obj[i]['predictions'][0],
            target=json_obj[i]['target'][0])
        for i in range(len(json_obj))]
    padding = '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'
    data = [d for d in data if padding not in d['prediction']]
    for d in data:
        p = d['prediction']
        trace_lines = get_tag('program_trace', p)
        boxed_answer=get_tag('answer', p)
        answer = get_answer(boxed_answer)
        correctness = is_correct(answer, d['specifics'], d['target'])
        d.update(
            output=trace_lines.split('\n'),
            boxed_answer=boxed_answer,
            answer=answer,
            is_correct=correctness)
    return data

def is_correct(answer, specifics, target):
    try:
        a = float(answer)
        lo = float(specifics['lower_limit'])
        hi = float(specifics['upper_limit'])
        return lo <= a <= hi
    except ValueError:
        None
    return False

def which_kind(d):
    rule_calc_names = medcalc.utils.medcalc_rules.rules.keys()
    formula_calc_names = medcalc.utils.medcalc_formulas.formulas.keys()
    trace = ''.join(d['output']).lower()
    for name in rule_calc_names:
        if name.lower() in trace:
            return 'rule'
    for name in formula_calc_names:
        if name.lower() in trace:
            return 'formula'
    return None

if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog=sys.argv[0].replace(".py", ""))
    parser.add_argument(
        '--json_file',
        required=True,
        help='details json log to load')
    parser.add_argument(
        '--calc_type',
        help='specify as "formula" or "rule" to filter')
    
    args = parser.parse_args()
    print(args)
    data = load_json_details(args.json_file)
    if (args.calc_type is not None):
        print(f'before filtering by {args.calc_type}', len(data))
        data = [d for d in data if which_kind(d) == args.calc_type]
        print(f'after filtering by {args.calc_type}', len(data))
    
    runner = audit.Runner(data, AuditMedcalcRules, do_eval=False)
    runner.run()
    dfs = runner.report(verbose=1)
    

3/24
 - ideas: maybe better to write my own test case class
   - t.__class__.__dict__.keys() --> methods defined for t
     - could just run all these and see if they 'pass'
   - unnittest-like functionality needed is just 'confirm' which would
     - check a boolean condition and msg, add "msg" to instance-level pass or fail counters
     - eg self.confirm("exactly one 'analyze_sentence' step",
                       len(trace.df[...]) == 1)       	  		  
   - runner could collect counters from all runs in a df

3/22
 - py trace_auditor.py sports_understanding --config1 conf.d/bbh.conf --config2 conf.d/sonnet3.conf  --hi 30

12/6
 - Some sampleÂ rates:
   - Anthropic Claude 3.5: $15/Mtok output
   - Open AI o1: $60/Mtok output
   - Smallish eval sets are 3Mtok
     - so maybe $50/eval on Claude, $200 on o1
     - $35,246 / Claude is 2349.73 Mtok
       - about 783.244444444 3Mtok evals
       - 

11/15
 Major code reorganization!
 - new requirement: configargparse (pip install this)
 - argutil now takes config file parameters: eg --config conf.d/bbh.conf to use BBH tasks
 - conf.d is a place to store useful configs (hopefully partly replacing the Makefile)
 - argutil was moved to arg_util
 - logutil was moved to log_util
 - a number of less-used python files have been moved to subdirectories
 - training/test data, cot-prompts, mocks, and task-specific code have been
   moved to task-specific subdirectories, like bbh, gridw, or synthetic

11/11
 Updated grid_logutil and gridw_expt a little
 Ran new gridworld with random destinations
 - py run_eval2.py hbargridw --hi 30 --model claude-3-5-sonnet-20241022 --example_dir synthetic/train
   Final totals correct=22 total=30 acc=0.7333333333333333
   excess_len 	 	 0.06666666666666667
   obst_crossed 	 0.23333333333333334
   dist_to_goal 	 0.0
   acc 	 		 0.7333333333333333
   possible_actions_ex 	 7
   optimal_actions_ex 	 2
   _examples 	 	 9
 - py run_eval2.py hbargridw --hi 30 --model claude-3-5-sonnet-20241022 --baseline_template_format --template_file synthetic/cot/hbargridw.txt --log_stem baseline-cot-dev --example_dir synthetic/train/
   Final totals correct=12 total=30 acc=0.4
   time py gridw_expt.py hbargridw --cot_json synthetic/cot/hbargridw.json --mock_file mocks2/hbargridw.py --model claude-3-5-sonnet-20241022 --example_dir synthetic/train/ --summarize_eval --num_iterations 6
       excess_len  obst_crossed  dist_to_goal  acc  possible_actions_examples  _examples  optimal_actions_examples
    0         0.0           0.4           0.0  0.7                          3          3                       NaN
    1         0.4           0.1           0.4  0.6                          5          7                       2.0
    2         0.0           0.1           0.2  0.8                          6          8                       2.0
    3         0.0           0.1           0.0  0.9                          7         10                       3.0
    4         0.0           0.2           0.0  0.8                          9         13                       4.0
    5         0.0           0.2           0.0  0.8                         11         17                       6.0
   rerun baseline (before learning): time py run_eval2.py hbargridw --hi 100 --model claude-3-5-sonnet-20241022 --example_dir synthetic/test 
   Final totals correct=61 total=100 acc=0.61
   excess_len 	 0.08
   obst_crossed 	 0.28
   dist_to_goal 	 0.03
   acc 	 0.68
   possible_actions_examples 	 28
   optimal_actions_examples 	 10
   _examples 	 38
   
   rerun after learning: time py run_eval2.py hbargridw --hi 100 --model claude-3-5-sonnet-20241022 --example_dir synthetic/test --partial_program_dir ../doctest-prompting-data/logs2/anthropic-claude-3-5-sonnet-20241022/hbargridw/gridw_expt_pps/subdir_050_060
   Final totals correct=62 total=100 acc=0.62
   excess_len 	 0.04
   obst_crossed 	 0.22
   dist_to_goal 	 0.14
   acc 	 0.77
   possible_actions_examples 	 20
   optimal_actions_examples 	 10
   _examples 	 30

11/8

 - Better arg structure  
   examples:
     example_dir, split, task
     lo, hi
     *function: args => examples
     *function: args => split_filepart (e.g., 'ptp-dev')
   llm:
     service, model 
     *function: args, prompt => output
     *function: args => llm_filepart(eg 'anthropic-haiku-20241012')
   prompted_model:
     template_dir, template_file
     template_format = [cot, ptp]
     (partial_program_dir | mock_dir), task, variant
     *function: args => prompt_template
     *function: args => task_filepart (e.g., 'sports_understanding/_baseline-dpt')
   runner:
     delay, parallel

 - Current arg structure run_util
   - log_util.add_args(parser):
     task, variant
     example_dir, lo, hi
   - run_util.add_args(parser):
     partial_program_dir
     log_dir
     log_stem
     template_file
     CoT, baseline_template_format
     parallel   
     delay
 - llm_util.add_args(parser)
     service, model
   

11/2
 - what's next?
   - haiku performance? that would be faster/cheaper
     run: time py gridw_expt.py hbargridw_77 --cot_json synthetic/cot/hbargridw_77.json --mock_file mocks2/hbargridw_77.py --model claude-3-haiku-20240307 --example_dir synthetic/train/ --summarize_eval --num_iterations 6 --force
     - performance on first 10: 3/10, mostly not producing a trace

   - duplicate removal in 'minibatch'
   - CLI reworking
   - gridw with random endpoints
   - gridw with 3 rooms?
   - gridw with obstacle-free CoT?
   - logical_deduction_K_objects for K=5, 7?
   - another synthetic task with learnable subtasks?
     - https://arxiv.org/pdf/2305.14879 - bytesized32 text games
   - grid_expt like model with psoracles
     - run K examples -> log1
     - run same K examples with output Y and Y-augmented trace -> log2
  - logic with possible/optimal actions

11/1 gridworld
  - Manually fixed top-level name in subdir_050_060
     Final totals correct=76 total=100 acc=0.76
     > was (after learning) Final totals (ignoring parse failures) correct=64 parsed=95 acc=0.6736842105263158
       with 5 parse failures so acc=0.64 with them
     > was (before learning) Final totals correct=72 total=100 acc=0.72

 - Looks like that bug is significant.  Reran incremental learning
   time py gridw_expt.py hbargridw_77 --cot_json synthetic/cot/hbargridw_77.json --mock_file mocks2/hbargridw_77.py --model claude-3-5-sonnet-20241022 --example_dir synthetic/train/ --summarize_eval --num_iterations 6 --force
       excess_len  obst_crossed  dist_to_goal  acc  possible_actions_examples  _examples  optimal_actions_examples
    0         0.0           0.6           0.1  0.9                          1          1                       NaN
    1         0.0           0.4           0.2  0.5                          5          8                       3.0
    2         0.1           0.1           0.0  0.9                          6          9                       3.0
    3         0.0           0.1           0.2  0.8                          7         10                       3.0
    4         0.0           0.2           0.0  0.9                          8         12                       4.0
    5         0.0           0.1           0.0  0.9                          9         13                       4.0

  - rerun baseline (before learning): time py run_eval2.py hbargridw_77 --hi 100 --model claude-3-5-sonnet-20241022 --example_dir synthetic/test 
  > Final totals correct=84 total=100 acc=0.84 [baseline-dpt-test-000-100.log]
  excess_path_len      0.00
  distance_to_goal     0.09
  obstacles_crossed    0.16
  perfect              0.84
  - reran midpoint: time py run_eval2.py hbargridw_77 --hi 100 --model claude-3-5-sonnet-20241022 --example_dir synthetic/test --partial_program_dir ../doctest-prompting-data/logs2/anthropic-claude-3-5-sonnet-20241022/hbargridw_77/gridw_expt_pps/subdir_020_030
  > Final totals correct=83 total=100 acc=0.83
  excess_path_len      0.17
  distance_to_goal     0.07
  obstacles_crossed    0.14
  perfect              0.87
  - reran midpoint: time py run_eval2.py hbargridw_77 --hi 100 --model claude-3-5-sonnet-20241022 --example_dir synthetic/test --partial_program_dir ../doctest-prompting-data/logs2/anthropic-claude-3-5-sonnet-20241022/hbargridw_77/gridw_expt_pps/subdir_030_040/
  > Final totals correct=84 total=100 acc=0.84
  excess_path_len      0.00
  distance_to_goal     0.09
  obstacles_crossed    0.07
  perfect              0.92
- reran last version (after learning, and before duplicates): 
  - running last version (after learning, w/o removing duplicates)  
    - note: start with $140 in anthropic funds, $136 after, so $4/100 queries.
  > Final totals correct=91 total=100 acc=0.91 [subdir_050_060-dpt-test-000-100.log]
  excess_path_len      0.00
  distance_to_goal     0.01
  obstacles_crossed    0.06
  perfect              0.94
  - reran last version (after learning, and after removing duplicates): 
  > Final totals correct=78 total=100 acc=0.78 [manual-dpt-test-000-100.log]
  excess_path_len      0.010204
  distance_to_goal     0.204082
  obstacles_crossed    0.051020
  perfect              0.908163
  - reviewing microtraces
    - @traced line is duplicated (?)
    - 4x examples of possible_actions((3, 7)) in second iteration, 1 more in 4th, so part of problem is just



10/31
 - gridworld:

   - ran: py gridw_expt.py hbargridw_77 --cot_json synthetic/cot/hbargridw_77.json --mock_file mocks2/hbargridw_77.py --model claude-3-5-sonnet-20241022 --example_dir synthetic/train/ --summarize_eval --num_iterations 6

   excess_len  obst_crossed  dist_to_goal       acc  _examples  possible_actions_examples  optimal_actions_examples
0         0.0      0.000000      0.600000  0.900000          0                        NaN                       NaN
1         0.1      0.500000      0.200000  0.400000          8                        5.0                       3.0
2         0.2      0.400000      0.000000  0.600000         14                        9.0                       5.0
3         0.0      0.333333      0.222222  0.666667         17                       12.0                       5.0
4         0.0      0.200000      0.000000  0.900000         19                       13.0                       6.0
5         0.0      0.700000      0.100000  0.800000         21                       15.0                       6.0

 - result: no clear trend with the 10 batch accuracies.  By inspection, a lot of duplicate errors, so it's not learning
 from the subtraces well.
 - to see if this is getting any better, ran:
  -  time py run_eval2.py hbargridw_77 --hi 100 --model claude-3-5-sonnet-20241022 --example_dir synthetic/test
     Final totals correct=72 total=100 acc=0.72
  - time py run_eval2.py hbargridw_77 --hi 100 --model claude-3-5-sonnet-20241022 --example_dir synthetic/test --partial_program_dir /Users/wcohen/Documents/code/doctest-prompting-data/logs2/anthropic-claude-3-5-sonnet-20241022/hbargridw_77/gridw_expt_pps/subdir_050_060
     Final totals (ignoring parse failures) correct=64 parsed=95 acc=0.6736842105263158

 - cleaning up configuration stuff

   command-line tools:
   option 1: https://pypi.org/project/ConfigArgParse/ allows config
   files to be used to set command-line options, as drop-in replacement
   for argparse.
   option 2: click https://click.palletsprojects.com/en/stable/
   also Fire, ...

   config tools:
   option 3: gin-config
   option 4: also Hydra, ...

   thinking thru this...
   use ConfigArgParse

10/28

 - grid world: gridw_expt now working adding microtraces in rounds.
   Running 8 8-example rounds with microtraces on hbargridw_77 I got
   this result (note the last round only 4 examples).

   % py gridw_expt.py hbargridw_77 --cot_json synthetic/cot/hbargridw_77.json --mock_file mocks2/hbargridw_77.py --model claude-3-5-sonnet-20241022 --example_dir synthetic/train/ --summarize_eval --num_iterations 8

   excess_len  obst_crossed  dist_to_goal       acc  possible_actions_examples  optimal_actions_examples  _examples
0         0.0         0.250         0.125  0.750000                          2                         1          3
1         0.0         1.000         0.000  0.714286                          4                         2          6
2         0.0         0.125         0.000  0.875000                          5                         3          8
3         0.0         0.125         0.000  0.875000                          6                         3          9
4         0.0         0.125         0.250  0.875000                          7                         3         10
5         0.0         0.000         0.000  1.000000                          7                         3         10
6         0.0         0.000         0.125  0.875000                          7                         3         10
7         0.0         0.250         0.000  0.750000                          8                         3         11
  
  - inspecting, there are many duplicates of the same correction for possible_actions((3, 7)).  So the microtraces
  are not being attended to properly.  It seems to be a common mistake, when you start at the top, to just
  head east and walk through the barrier.

10/25
 - grid world
   - should back out the eval hooks and just write a grid_expt
     - 

10/22
 - worked on new gridworld
 - new version is cleaner but text is longer - should I try and compress it? Action(from=(x,y), to=(x,y), dir='n')
 - new grid is 8x8, goal at 7,7 and vbar from bottom to almost the top

10/21
  tasks for today:
  - new gridworld
  - read about vllm, etc

10/18

Suggestions for Zhixian initial train/test split:

  train = disambiguation_qa hyperbaton logical_deduction_three_objects  object_counting web_of_lies
  test = sports_understanding reasoning_about_colored_objects boolean_expressions tracking_shuffled_objects_three_objects temporal_sequences

William gridworld experiments:
 - first draft of gridworld working
   - easy: no obstacles
   - hbar: bar from 4,2 to 4,8.  w/ and w/o "obstacle instructions" (obstinst)
     that say where the obstacle is at the top-level.

   	     	    	   CoT      PTP
  easy		  	   0.9	    0.967		  
  hbar	     	  	   0.7      0.7
  hbar+obstinst   	   0.367    0.6
  _mt_implied_around	   ---	    0.567	implied microtraces that go "around", not in direct line 
  _mt_implied3		   ---	    0.6333	all implied microtraces (not the first 3)

  # better algorithm - separates world model (possible_actions), policy
  # (optimal_actions) and transition matrix (perform_action)

  state = (x0, y0)  # maybe remaining steps also?
  path = []
  while state != goal_state:
  	possible = possible_actions(state)  # possible actions, subset of (nsew, state')
	optimal = optimal_actions(actions, goal_state)  # minimal-distance state's
	action_name, state = perform_action(state, optimal[0])
	path.append(action_name)

  # task is 'navigate from x0, y0 to xn, yn in N steps' - N>=minimum distance
  # learning loop
  #   run PTP
  #   record success/failure, obstacles, path len, ... 
  #   use oracle to correct first error (imitation learning) by adding microtraces


10/16
 - in middle of updating makefile for gridw
 - also need to help generate mock!

10/15
  - fixed a bug in the code (simplify generated incorrect predicate/constant names) but it is NOT better
    - old test set formal_fallacies_nltk vs baseline
        binomial test: p=0.1608 BinomTestResult(k=20, n=51, alternative='two-sided', statistic=0.39215686274509803, pvalue=0.16077960181198847)    
    	On jointly parsed examples :    acc1=0.568 acc2=0.639 delta=-0.071
	overall 62.1 vs 56.8 (or 46.3 with parse failures - cot is 57.4)
    - NEW test set formal_fallacies_nltk vs baseline
        binomial test: p=0.8939 BinomTestResult(k=27, n=56, alternative='two-sided', statistic=0.48214285714285715, pvalue=0.8938530948350216)
	On jointly parsed examples :    acc1=0.555 acc2=0.568 delta=-0.013


10/11

 - running: py analysis_scripts/oracle_pair_report.py --pattern '../doctest-prompting-data/special-logs/oracle_utrace-logs/*-mod.json'
	[np.float64(0.03125), np.float64(0.125), np.float64(0.21875), np.float64(0.03125), np.float64(0.625), np.float64(0.03125)] 
	['update_stack', 'empty_stack', 'is_simple_expression', 'rewrite_expression', 'eval_expression', 'bucket_sort']
	[False False False False False False] [0.17344779 0.33007812 0.38964844 0.17344779 0.625      0.17344779]
 - this shows no significant differences with a multiple test correction, but only testing up to 30 runs of each step
 
Are my modularity tests too small? they are dev test for NLP, and for oracle steps, I just take 30 for each step from the test set.
Maybe I should do bugger sample, say 100?
Proposed text: "Testing difference when there is an oracle is easy since you can just do a direct paired test on step error rates."
 


10/1
  - running a bunch of variants of formal_fallacies with proofs are traces
    - best variant so far is formal_fallacies_nltk with PROVER=='resolution'
      - nltk_tuned version on test set is worse (59.5% vs 62.1%)
    - on dev set formal_fallacies_nltk vs baseline: 
        binomial test: p=0.4531 BinomTestResult(k=2, n=7, alternative='two-sided', statistic=0.2857142857142857, pvalue=0.453125)
    	On jointly parsed examples :    acc1=0.682 acc2=0.818 delta=-0.136
    - on test set formal_fallacies_nltk vs baseline
        binomial test: p=0.1608 BinomTestResult(k=20, n=51, alternative='two-sided', statistic=0.39215686274509803, pvalue=0.16077960181198847)    
    	On jointly parsed examples :    acc1=0.568 acc2=0.639 delta=-0.071
	overall 62.1 vs 56.8 (or 46.3 with parse failures - cot is 57.4)
    - summary: promising but not there yet.  possible extensions:
      - extend traces to explain what's going on (as in my resref algorithm)
        - maybe just with a prompted model
      - add more example traces

9/30 left to do

 - paper cleanup
   - waiting for cassie's edits

 - code cleanup
   - have cassie check in other annotations?
   - collect requirements and make a package
   - get rid of deprecated code/scripts/mocks
     - including example_util
   - expt_util - rename intervention_expt, tests
   - job_util - can we make prompt template a datafile/argument? what is gemma's prompt?
   - llm_util.py - remove deprecated code, clean up 
   - logutil.py - misc clean ups
     - data structure returned by parse_output
     - move the reporting into a separate file in analysis_scripts
   - oracle_util - move report to separate file
   - pairwise_log_cmp, threeway_log_cmp - move to analysis_scripts
   - mocks2: document or deprecate all mocks

 - llama 70B/ 405B / gemma27B results
   - together.ai "works" but results are way worse than cassie's
   - running together.ai llama 8B-reference as another check
     results still worse than Cassie
   - plan: see if we can get the vLLM stuff working, or have some
     other plan for running Llama 70B on Babel


9/25 geometric_shapes for grant
	weirdness: phase 2 happens almost 1/2 the time
	geometric_shapes - psoracle_expt on test with geometric_shapes_ctr0_psoracle

	summarize_decomposed_path	& \multicolumn{2}{c}{options_matching_summary}	compute\_length_clusters options\_matching\_clusters	& Macro Avg & Micro-avg \\
	(early step)			& \multicolumn{2}{c}{(later step)} &		& (optional early step) & (optional late step) & & \\
	$k=9, n=16 (56\%)$	&	$k=57, n=64 (89\%)$ & $p<0.001$ & 		& $k=4, n=5 (80\%) 	& $k=11, n=15 (73\%) & 	74\% &	  81\% \\
test	9 / 16 56.25			57 / 64	89.1	p < 0.001	4 / 5	80		11 / 15	 73.3			74.25	  81


should be:
       order / step
       1 summarize_decomposed_path*
       2 options_matching_summary*
       3 compute_length_clusters*
       4 length_clusters_match_option*

9/22
   - asked cassie to annotate _ctr0

9/20
   - geometric_shapes tuning experiments - on test

   _ctr has one instruction/example already though!
   _ctr0 has no instructions
   _ctr0 has 1 non-local and 9 local errors
   baseline had 4 non-local errors and 10 local errors

   log1  acc1  log2 	   acc2   p
   
   cot	 53.3  dpt  	   40.0   0.003	#p<0.01 missing from table?
   cot	 51.6  ctr0  	   53.8   0.689
   dpt	 40.9  ctr0	   55.9	  0.006

   conclusion:   dpt < [cot, ctr0]

   ctr0	 56.3  ctr	   62.6	  0.135
   ctr0	 54.1  ctr_tuned   71.6	  0.001
   ctr	 62.3  ctr_tuned   70.9	  0.063

   conclusion: ctr0 < ctr_tuned
   	       ctr is probably in-between

   cot	 52.2  ctr_tuned   70.4	  0.002

   conclusion: cot < ctr_tuned 

   dpt	 40.7  ctr_tuned   70.1	  0.000
   cot	 52.8  ctr  	   61.2	  0.058
   dpt	 40.9  ctr	   62.6	  0.000

   another summary:

   acc* ignoring parse failures, unpaired
   acc  counting parse failures as failures, unpaired

   log	       acc*	acc	p>prev p>ctr0
   dpt	       40.0	37.9	
   cot	       51.6	51.6	0.003	
   ctr0	       53.7	52.6	0.688	
   ctr	       61.2	57.3	0.135
   ctr_tuned   70.4	68.9	0.063	0.001



9/18
 - approximate token sizes with new analyze_size routine

   all test    2.8M tokens
   avg test    122k tokens
   all dev     0.5M tokens
   SAMPLE dev  125k tokens

9/16
 - sports_understand_notmodular1 experiment
   py -i analysis_scripts/replace_call_permtest.py --log_pattern '../doctest-prompting-data/special-logs/modularity-logs-v1/sports_notmod*1-logs/*.log'

				acc	 avglen %identical
   baseline normal execution	97.4	 4.47		#on dev
   baseline forced execution	97.4	 4.47  		#on dev

   nonmod1 normal execution: 	98.4	 4.30  		#test
   nonmod1 forced_modularity:	97.4	 4.65  	56.8	#test

   nonmod2 normal execution:	99.5	 4.30		#test
   nonmod2 forced_modularity:	81.0	 4.87  	46.8	#test

   Narrative:
    - what do steps mean? do they behave as we think?
      - can steps be executed in isolation?
        - oracle result: good accuracy
      - do steps behave the same in isolation (modular)?
        - oracle result: isolated accuracy < in-trace accuracy 
      - can we measure modularity of general steps?
        - why is measuring modularity hard?
	  - example: s/consistent_sports(s1, s2)/consistent_sports(s2)
	    - result: 
	      - acc not lower! actually a little higher!
	      - doesn't need inputs to be marked, LLM can use anything
	  - measuring modularity with interventions 
	    - as in Lanham et al, make a change and observe impact on accuracy
	    - describe process of "forced modularity intervention"
	    - result: 
	      - still not lower
	      - qualitatively, wants to redo the test
	      - reflected in longer avg # steps (not accuracy)
	    - discussion: self-consistency/robustness
	 - a more precise view of intervention-based measurement
	   - stable incremental gen assumption
	   -  t, ~t, s, ~s
	   - oracle result table
	   - general result table

9/12
 - quick probe of llama_api - it's outputs are very short

9/11

What I have:
 - dev .log, dev.baselog for modularity_utrace-logs/ 
   [some oracle_tasks are mixed in?]
 - dev .log, dev.baselog for modularity_utrace-oracle-logs/
   [a few are missing, running now]

Discussion:
 - robustness isn't bad
 - modularity isn't all good

Story seems to look good for modularity:

 - incremental generation assumption seems safe, possibly not for longest/hardest
   program induction problems
 - non-modularity cannot be detected with errors alone 
 - further non-modularity is rare with abstract traces, n_steps, n_chars 
   - the stats find different cases and n_chars is a pretty good baseline
 - might see more non-modularity with more test cases 
   - but it seems to generally be a good abstraction

In this framework, inference is more or less step-by-step according to sensible
plans.

 
\begin{tabular}{llrl}
\toprule
 & statname & p_lt_0.05 & tasks \\
\midrule
0 & n_unbalanced & 0 & Counter() \\
1 & n_steps & 2 & Counter({'salient_translation_error_detection': 2}) \\
2 & skeleton & 2 & Counter({'salient_translation_error_detection': 2}) \\
3 & is_correct & 0 & Counter() \\
4 & n_chars & 2 & Counter({'snarks': 2, 'ruin_names': 2}) \\
\bottomrule
\end{tabular}


 - done yesterday: modularity_expt w
    - DONE modularity_utrace-logs/ dev [38]
    - DONE modularity_utrace-logs/ baseline
    - TODO modularity_utrace-logs/ tune
    - TODO modularity_utrace-logs/ baseline tune

    - TODO modularity_utrace-oracle-logs/ dev [11]
      - need dev log for word_sorting [run], tracking_shuffled_objects_three_objects [run]
    - TODO modularity_utrace-oracle-logs/ dev baseline [11]
    - TODO modularity_utrace-oracle-logs/ tune [11]
      - need tune logs for boolean_expressions [run], dyck_languages [there?], multistep_arithmetic_two [run]


  - oracle_tasks _utrace baseline
  - oracle_tasks _utrace baseline tune
  - oracle_tasks _utrace tune

9/10

 - weird result: baseline with oracle steps seems to treat
 is_open_paren as non-modular, I suspect not properly dealing with
 **parse failed** inputs.

 - to do
   - modularity_expt with new expt_util on tune and baselog
   - modularity_expt for oracle steps - 

9/9
 - reran modularity_expt with new expt_util
 - added some extra opts for replace_call_report
 - results similar: snarks.judge_statement is suspiciously non-modular but not stat sigificantly so

9/5
  - updated expt_util (replace call) to use the new single-step prompting and
  extraction framework

9/4
  - Next steps for Cassie's experiments with local models
    - scale up: try all the llama sizes you can, ask Jao if you get stuck
    - rewrite to save logs in json, and view them with something different
    - try out gemma models
    - try a raw, non-instruction trained model with 'continuation prompting'

  - For me:
    - another experiment: redo modularity logs with utraces and new trace extraction.
      *_utrace.py programs with comments about what to trace are in the mocks2/ directory

   - try the sample tasks/dev on: 
      - anthropic sonnet 3.5
      - anthropic sonnet 3 (if needed)
      for i in $SAMPLE_TASKS; do if ! [ -f $DIR/$i/baseline-dpt-dev.log ]; then make $i.devdpt; fi; done
      - openAI mini GPT4o
      - openAI GPT4o
      - Gemini-Pro
      - Llama 3.1 405B

  - questions to consider: 
    - does the feature "# calls to the replaced step" reliably detect non-modularity?
    - does non-modularity correlate with skeleton entropy?
      sports_understand_notmodular experiments suggests higher-entropy
      tasks might be more prone to adaptation
    - does non-modularity correlate with direct-prompting performance? 
      maybe when the model "already knows" steps are less important to follow.

 - project ideas

   - look more at training with utraces
     - artificial datasets?
       - simplified flag colors
       - hyperbaton but with made-up adjectives

   - try some harder tasks (eg condQA, ...) with PTP
     - maybe there are some associated with ITS tasks
     - tree of thoughts / alg of thoughts tasks

   - explore tasks with overlapping steps and item response theory, ITS data

   - look at ps-oracles and other approaches to symbolic gradient

   - automate CoT => PTP, fine-tune for PTP
     - fine-tune for modular results?
     - CoT or PTP => top-level function

   - combine with agents / tools
     - do LLM agents work better with upstream/downstream context passed in?
     - can DsPy optimization methods be used for steps in PTP?
       - when do utraces help?
     - can you distill tools into a PTP prompt?

9/3
 - worked a little on oracle experiments:
   - moved code to extract step outputs from oracle_util to debugger
   - wrote mock.py code for ###DOCTRACES FOR foo IMPLIED BY bar
   - now I need to re-run the experiments

Summary: 
				ptp	mod	mod-utrace-3
	 original extraction	97.5	87.1	87.9
	 first call extraction	97.5	86.9	91.0

	 Within PTP trace		97.6
	 Step-specific Prompt		91.0	-6.6
	 ~~ $-$ trace parsing		87.9	-9.7
	 ~~ $-$ micro-traces		86.9	-10.7



	 py analysis_scripts/oracle_report.py --compare '../doctest-prompting-data/special-logs/oracle-logs/*ptp.json' --compare_tag ptp --compare '../doctest-prompting-data/special-logs/oracle-logs/*mod.json' --compare_tag mod
	 acc.ptp 0.9750000000000001
	 acc.mod 0.86875

	 oracle_utrace
	 acc.ptp 0.9750000000000001
	 acc.mod 0.9104166666666667

	 oracle-logs-v1
	 acc.ptp 0.9750000000000001
	 acc.mod 0.8708333333333333

	 oracle_utrace-logs-v1
	 acc.ptp 0.9750000000000001
	 acc.mod 0.9104166666666667


9/2
 - added better outline to my sync doc

8/31
 - qualitative analysis of rewrite_expression
   - adding multiple [] - a few (1-2)
   - generate sequence of rewrites - many, checking for FIRST output might help a lot
     - does this also happen with NLP tasks???
   - a quick check looking for the FIRST output - with 4 utraces + instructions
 . ptp correctness Counter({True: 26, False: 4})
 . mod correctness Counter({True: 17, False: 13}) -- up from mod correctness Counter({False: 21, True: 9}) w/o first-output check
 . . Counter({'total': 30, 'different': 13, 'ptp_win': 11})
 ====
 thought: could repeating operations be like a sort of self-consistency? if you are more likely to be right than wrong
   repeating a step K times and taking the last might reinforce correct iterations....?

8/30
 - cassie 
    - using charlen(prompt)/6 as a token bound
    - mostly running now, except for some issues with log
    - llama 3.1 8B cot and ptp baseline for 6 sample tasks - about 4hrs for six tasks for both cot and dpt
      - varies a lot based on output length
      - using A6000 GPU's

 - started converting the modularity experiments to micro-traces, but
 stopped because ... I also tweaked oracle_report.  Adding one trace
 helps, but not that much overall - it clears up the syntax errors but
 not all the mistakes.  Might want to consider adding K u-traces, but
 maybe that should be automated? 
 eg: python task.py prompt --num_microtraces K --microtrace step1 --microtrace step2 ...

     acc.mod 0.8708333333333333
     acc.mod_utrace 0.8791666666666667
     acc.ptp 0.9750000000000001
     acc.ptp_utrace 0.9750000000000001

     syntax.mod 0.9625
     syntax.mod_utrace 1.0
     syntax.ptp 1.0
     syntax.ptp_utrace 1.0

 - to diagnose, added all the rewrite_expression utraces from multistep_arithmetic_two - this
 is by far the worst one - and reran it.

  python3 oracle_util.py multistep_arithmetic_two.py --test_set --max_step_sample 30 --check_modular_outputs --partial_program_dir mocks2/partialprograms_utrace --output_dir tmp_logs

results for step rewrite_expression:
 . ptp correctness Counter({True: 28, False: 2})
 . mod correctness Counter({False: 21, True: 9})
 . result comparison:
 . . Counter({'total': 30, 'different': 19, 'ptp_win': 19})

 with 0 mtraces - doctest-prompting-data/oracle-logs/multistep_arithmetic_two-mod.json
    "rewrite_expression": {
      "false": 18,
      "SyntaxError": 11,
      "true": 1
    },
 with 1 mtrace - doctest-prompting-data/oracle_utrace-logs/multistep_arithmetic_two-mod.json
    "rewrite_expression": {
      "false": 25,
      "true": 5
    },
 with 4 mtraces: ({False: 21, True: 9})

 with 4 traces and no instructions:
 . mod correctness Counter({False: 26, True: 4})
 . result comparison:
 . . Counter({'total': 30, 'different': 22, 'ptp_win': 22})
 with 4 traces interleaved with instructions:
 . mod correctness Counter({False: 21, True: 9})
 . result comparison:
 . . Counter({'total': 30, 'different': 19, 'ptp_win': 18})

 conclusion: nothing helps much, but more traces is better, and the
 instructions help.  I don't know why doing this step in context is so
 much better than doing it independently.

8/29
 - setup oracle_util to compare individual steps to oracles
   - boolean_expressions. 
   - dyck_languages test set works
   - multistep_arithmetic_two.eval_simple_expression sometimes doesn't give valid answers.
   - tracking_shuffled_objects_three_objects works
   - word_sorting works
 - running with max of 30 samples of each step: overall over the five tasks
   - modular execution is correct according to oracle 84.4% and syntactically ok 93.5%
   - incontext execution is correct according to oracle 97.5% and syntactically ok 100.0%

 - can we do better??? eg would adding 1-2 microtraces for each oracle step help?
 - does this syntax error crap affect modularity?

8/28
 - extended oracle_util to run individual steps and report their accuracy
 (for now, vs the end2end step output, not vs the oracle)

8/27

 - Wrote oracle_util which checks the output of steps against an
 oracle for a few steps (17 in 5 tasks).  Next steps would be to
   - do the same measurements for modular replacement steps in the dev set
   - annotate some subset of the dev set input/output pairs for "reasonableness"

 py oracle_util.py dyck_languages --test_set
    is_open_paren Counter({True: 3857})
    update_stack Counter({True: 3849, False: 7, 'SyntaxError': 4})
    empty_stack Counter({True: 162, False: 3})

 py oracle_util.py boolean_expressions --test_set doesn't converge
  - the boolean_expressions have infinite loops in general

 py oracle_util.py multistep_arithmetic_two --test_set
    is_simple_expression Counter({True: 1797, False: 23})
    rewrite_expression Counter({True: 1324, False: 116, 'SyntaxError': 7})
    eval_simple_expression Counter({True: 378, False: 2})
    eval_expression Counter({True: 355, False: 25})
    eval_variabilized_expression Counter({True: 190})
  
 py oracle_util.py tracking_shuffled_objects_three_objects --test_set
    simulate_swap Counter({True: 568, False: 2})
    answer_question Counter({True: 190})

 py oracle_util.py word_sorting --test_set
    kth_letter Counter({True: 3551, False: 4, 'SyntaxError': 2})
    partition_words Counter({True: 817, False: 11, 'SyntaxError': 1})
    sort_keys Counter({True: 826, False: 1})
    bucket_sort Counter({True: 2751, False: 111, 'SyntaxError': 9})
    flatten Counter({True: 742})

 - Perm test summary: non-modularity is invisible if you just look at the final outputs,
   but common (not ubiquitous) - none in formal_fallacies or hyperbaton, eg

 - Perm test on baselog: n_steps and skeleton for sports_understand-consistent_sports-n1 is slighty
   significant, but that's at the very end of the trace.
     n_unbalanced multiple test: any(reject)=False alpha_bonf=0.0013157894736842105
     tested 38 subtasks
     number significantly different: Counter()
     Perm test on n_steps:
     n_steps multiple test: any(reject)=True alpha_bonf=0.0013157894736842105
     0.030*   0.001 ../doctest-prompting-data/modularity-logs/sports_understanding-consistent_sports-n1-dev.baselog
     tested 38 subtasks
     number significantly different: Counter({0.05: 1})
     Perm test on skeleton:
     skeleton multiple test: any(reject)=True alpha_bonf=0.0013157894736842105
     0.045*   0.001 ../doctest-prompting-data/modularity-logs/sports_understanding-consistent_sports-n1-dev.baselog
     tested 38 subtasks
     number significantly different: Counter({0.05: 1})
     Perm test on is_correct:
     is_correct multiple test: any(reject)=False alpha_bonf=0.0013157894736842105
     tested 38 subtasks
     number significantly different: Counter()
     Perm test on n_chars:
     n_chars multiple test: any(reject)=False alpha_bonf=0.0013157894736842105
     tested 38 subtasks
     number significantly different: Counter()

 - Perm test on logs: summary
     with n_steps: 20/38 are different at the 95% level 
     with skeleton: 25/38 are different at the 95% level 
     with correct: none are different
     with n_chars: 2/38 are different

     Perm test on n_unbalanced:
     n_unbalanced multiple test: any(reject)=False alpha_bonf=0.0013157894736842105
     tested 38 subtasks
     number significantly different: Counter()
     Perm test on n_steps:
     n_steps multiple test: any(reject)=True alpha_bonf=0.0013157894736842105
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-summarize_statement-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/movie_recommendation-explain_best_choice-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/disambiguation_qa-is_interpretation_logical-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/movie_recommendation-summarize_movies-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/sports_understanding-sport_for-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/disambiguation_qa-is_interpretation_logical-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/movie_recommendation-movie_properties-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-summarize_statement-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/sports_understanding-sport_for-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/reasoning_about_colored_objects-query_colored_objects-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/date_understanding-make_inference-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-judge_statement-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-is_sarcastic-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-judge_statement-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/disambiguation_qa-find_possible_interpretations-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-is_sarcastic-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/salient_translation_error_detection-find_translation_error-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/ruin_names-meaningful_edit-n1-dev.log
     0.035*   0.002 ../doctest-prompting-data/modularity-logs/date_understanding-answer_question-n1-dev.log
     0.045*   0.002 ../doctest-prompting-data/modularity-logs/salient_translation_error_detection-choose_error_type-n1-dev.log
     tested 38 subtasks
     number significantly different: Counter({0.01: 18, 0.05: 2})
     Perm test on skeleton:
     skeleton multiple test: any(reject)=True alpha_bonf=0.0013157894736842105
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-summarize_statement-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/movie_recommendation-explain_best_choice-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/disambiguation_qa-is_interpretation_logical-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/movie_recommendation-summarize_movies-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/sports_understanding-sport_for-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/disambiguation_qa-is_interpretation_logical-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/movie_recommendation-movie_properties-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/ruin_names-meaningful_edit-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-summarize_statement-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/sports_understanding-sport_for-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/date_understanding-answer_question-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/reasoning_about_colored_objects-query_colored_objects-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/date_understanding-make_inference-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-judge_statement-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-is_sarcastic-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/date_understanding-make_inference-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/causal_judgement-relevant_sentences-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/causal_judgement-plausible_conclusion-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-judge_statement-n2-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/disambiguation_qa-find_possible_interpretations-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/snarks-is_sarcastic-n1-dev.log
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/salient_translation_error_detection-find_translation_error-n1-dev.log
     0.013*   0.001 ../doctest-prompting-data/modularity-logs/causal_judgement-plausible_inference-n1-dev.log
     0.018*   0.001 ../doctest-prompting-data/modularity-logs/penguins_in_a_table-answer_question-n1-dev.log
     0.036*   0.003 ../doctest-prompting-data/modularity-logs/salient_translation_error_detection-choose_error_type-n1-dev.log
     tested 38 subtasks
     number significantly different: Counter({0.01: 22, 0.05: 3})
     Perm test on is_correct:
     is_correct multiple test: any(reject)=False alpha_bonf=0.0013157894736842105
     tested 38 subtasks
     number significantly different: Counter()
     Perm test on n_chars:
     n_chars multiple test: any(reject)=True alpha_bonf=0.0013157894736842105
     0.008**  0.000 ../doctest-prompting-data/modularity-logs/movie_recommendation-explain_best_choice-n1-dev.log
     0.015*   0.000 ../doctest-prompting-data/modularity-logs/hyperbaton-classify_adjective-n2-dev.log
     0.049*   0.001 ../doctest-prompting-data/modularity-logs/hyperbaton-classify_adjective-n1-dev.log
     tested 38 subtasks
     number significantly different: Counter({0.05: 2, 0.01: 1})


8/26
 - added analysis_scripts/replace_call_fun.py which does a permutation tests
 on the statistics from a replace_call log:
   - n_unbalanced: number of unbalance skeletons
   - n_steps: number of steps
   - skeleton: distribution of skeletons
   - is_correct: accuracy

8/14

 - synthetic data -
   - did some runs, but the task was way to hard as I formulated it.
   joins are really hard for LLMs!?? maybe.
   - dropped back the number of counties to simplify, and added
   an explicit listing of country colors to the COT generation.
   - need to REDO THE MOCKS though, and the tuning...and re-test

 - somehow the sports_understand_notmodular stuff got lost, and restored,
 but the results should ideally be re-done

 - Haiku results: fine, as expected

       	                dpt        cot
  date_understanding	acc1=1.000 acc2=0.867 delta=0.133 
  hyperbaton	    	acc1=0.800 acc2=0.533 delta=0.267 
  sports_understand  	acc1=0.900 acc2=0.967 delta=-0.067
  geometric_shapes   	acc1=0.367 acc2=0.600 delta=-0.233
  multistep_arithmet 	acc1=0.767 acc2=0.667 delta=0.100 
  logical_deduction  	acc1=0.600 acc2=0.867 delta=-0.267
  			     0.739      0.750

  Deepseek-Coder			    
   		           dpt		cot
  date_understanding	acc1=0.933 acc2=0.933 delta=0.000 
  hyperbaton	    	acc1=1.000 acc2=1.000 delta=0.000 
  sports_understand  	acc1=0.900 acc2=0.933 delta=-0.033
  multistep_arithmet 	acc1=1.000 acc2=1.000 delta=0.000 
  geometric_shapes   	acc1=0.586 acc2=0.655 delta=-0.069
  logical_deduction  	acc1=0.933 acc2=1.000 delta=-0.067

 Gemini-Flash

 date_understanding acc1=0.767 acc2=0.933 delta=-0.167
 hyperbaton	    acc1=0.900 acc2=1.000 delta=-0.100
 sports_understand  acc1=0.967 acc2=1.000 delta=-0.033
 multistep_arithmet acc1=0.966 acc2=1.000 delta=-0.034
 geometric_shapes   acc1=0.759 acc2=0.724 delta=0.034
 logical_deduction  acc1=0.733 acc2=0.967 delta=-0.233

 - outline: do models progress step by step?
   - Turpin et al idea, and robustness issues
   - modularity results
   - limitation because of robustness
     - robustness result: sc for boolean_expressions, word_sorting ?
   - consistent_sports experiments
     - full test set with standard model
       - change in immediate vs final output
     - non-modular1 - turns out to be robust
       - change in immediate vs final output
       - can we detect changes in the abstract trace?
     - non-modular2 - turns out to be NOT robust
       - change in immediate vs final output

8/13

 - more bugs: geometric_shapes has print statements!

 - presentation:

 Move single-step prompting and trace completion to methods.

 Show trace_completion doesn't change results.

 Turpin et al propose measuring faithfulness by investigating the
 effect of counterfactual interventions on a CoT explanation (eg,
 truncating the explanation).  Turpin usually, testing if the final
 answer does not change, we look and see if the correctness changes.
 this is binary so we can do a simple stat test to see if a change
 in distribution is statistically significant.

 assume trace t_i is s^i_1 ... s^i_n where s^i_j is the trace of the j-th step.

 Let X_j be inputs to s_j and Z_j be outputs, and let W_{<=j} be all random vars before X_j,
 not including X_j 

 Final output of the whole thing is \hat{Y}^i and loss is L^i =
 I[hat{Y}_i = Y_i].

 modularity for step h means that
    Z_j is conditionally independent of W_{<=j} | X_j
    => L_i is conditionally independent of W_{<=j} | X_j

    [aside: this might be true anyway, eg if the step is unnecessary
    we could test for this with another replacement, maybe with an
    'oracle' that outputs a constant text]

 ****
  - snarks drilldown isn't complete, snarks-judge_statement-n1-test.log is only
    the first 61 examples....but still, looks like this is not a non-modular
    output.  I did find a bug in expt_util.py - replacements are not well-formed -	
    so really I should redo *everything* for modularity and pseudo_oracle

 - expts: wrote sports_understanding_notmodular which hides arg2 from
   the consistent_sports function, and ran on dev/tune (twice, after
   fixing the bug).  It's very accurate, almost no change in performance,
   and swapping out the consistent_sports doesn't make much difference.
   Maybe because it's happy to re-do a consistent_sports call when
   it completes the trace...? since they happen 2x in one of the cot
   examples?

   worse - 97.4 vs 97.9 accurate on test - so the model is
   surprisingly robust and (2) not statistically significantly
   non-modular p=0.096, p(worse|change)=0.722, but p(change) only
   0.072

8/12

   Insights summary
   - maybe snarks should combine steps, maybe, since non-modular
   - what about dyck_languages?
   - geometric_shapes also, since there are non-modular steps
   - ruin_names is buggy, and I maybe should fix that?
   - logical_deduction_three_objects is significantly worse
   - formal_fallacies might be impossible but performance is clearly bad

   Modularity draft

   Measuring modularity is more expensive than simply running the
   model, since it requires two additional inference for every step
   occurrence.  The steps in the algorithmic tasks typically do not
   require extra inputs to perform accurate, so we focused on the 12
   NLP tasks.  We manually chose 20 steps that performed semantically
   non-trivial operations and evaluated each on the dev and test sets.
   For some of the tasks, we ourselves to the first 2 occurrences of
   each selected step.

   As a baseline, we first performed a simpler version of the
   measurement method, where we simply truncated the trace $t_i$ after
   the last line of the $j$-occurrence of step $s$, and then completed
   that partial trace.  We measured this on all of the dev data.
   
   \begin{tabular}{lccc}
			& $\Pr(L'_{ij}\not=L_i)$ & $\Pr(L'_{ij} < L_i|L'_{ij}\not=L_i)$ & $p$ value\\
   \hline			
   Break and continue 	& 6.4\%  & 45.3\% & 0.532 $\gg$ 0.05 \\
   Break, replace and continue
   	  	   	& 10.3 \%  & 53.5\% & 0.428 $\gg$ 0.05 \\
   \hline
   \end{tabular}

    date_understanding=2,
    disambiguation_qa=2,
    formal_fallacies=2,
    hyperbaton=3,
    penguins_in_a_table=2,
    snarks=2,
    sports_understanding=2,


8/9
 - exploring annotated local_errors:
   task='multistep_arithmetic' n=15 [('PEMDAS', 6), ('mult_div', 4), ('rewrite_expression', 3), ('add_subtract', 1), ('parenthesize_negative_numbers', 1)]
   task='salient_translation_errors' n=11 [('find_translation_error', 4), ('choose_error_type', 4), ('choose_answer', 2), ('german_to_english', 1)]
   task='geometric_shapes' n=10 [('summarize_decomposed_path', 5), ('summary_matches_option', 4), ('relate_length_clusters_to_option', 1)]
   task='formal_fallacies' n=6 [('to_logical_form', 3), ('do_combine_action', 2), ('suggested_action', 1)]
   task='ruin_names' n=6 [('humorous_edit', 3), ('edited_name_judgement', 3)]
   task='movie_recommendation' n=5 [('explain_best_choice', 5)]
   task='logical_deduction_three_objectrs' n=5 [('is_consistent', 5)]

 - wrote a bunch of pseudo_oracle programs
   - geometric_shapes --- should I switch to the new one?
   - movie_recommendations
   - salient_translation_error_detection
   - should also do ruin_names but
      - ruin_names.py gets Cot example #3 wrong (!)
       - fixed _v2 to get it right, now re-running on --test_set
         - it is not better - about 5 pts worse (not significant
           though, p=9.8%)
      - not clear what program lead to current log....looks like the same
      one but with the cot output bug????

   - should do multistep_arithmetic, but annotations are stale
   - could do logical_deduction_three_objects



8/6
 Exploring other models

 - deepseek-coder does fine: on dev
   		           dpt		cot
  date_understanding	acc1=0.933 acc2=0.933 delta=0.000 
  hyperbaton	    	acc1=1.000 acc2=1.000 delta=0.000 
  sports_understand  	acc1=0.900 acc2=0.933 delta=-0.033
  multistep_arithmet 	acc1=1.000 acc2=1.000 delta=0.000 
  geometric_shapes   	acc1=0.586 acc2=0.655 delta=-0.069
  logical_deduction  	acc1=0.933 acc2=1.000 delta=-0.067

 - gpt-4o-mini does TERRIBLY with ptp: on --test_set
  date_understanding: cot 92.1 ptp 67.4
  geometric_shapes:   cot 65.3 ptp 32.6
  hyperbaton: 	      cot 96.8 ptp 68.1  # after 132 examples, then it fails
  logical_deduction:  cot 96.3 ptp 74.2
  multistep_arithm:   cot 90.5 ptp 68.4
  sports_understand:  cot ???? ptp 82.1	 #cot failed but still..

 - on sample tasks, gemini-flash, dev set: PTP still loses by a lot 
  date_understanding acc1=0.767 acc2=0.933 delta=-0.167
  hyperbaton	    acc1=0.900 acc2=1.000 delta=-0.100
  sports_understand  acc1=0.967 acc2=1.000 delta=-0.033
  multistep_arithmet acc1=0.966 acc2=1.000 delta=-0.034
  geometric_shapes   acc1=0.759 acc2=0.724 delta=0.034
  logical_deduction  acc1=0.733 acc2=0.967 delta=-0.233

8/5
 - openAI gpt-4o-mini was 9min for --hi 30, so maybe 1hr per task for --test_set
   kicked off gpt-4o-mini runs on a small subset of tasks
     - NLP: date_understanding, hyperbaton, sports_understanding (lose/win/tie)
     - ALG: multistep_arithmetic_two, geometric_shapes, logical_deduction_three_objects (win/lose/tie)
   note: 
     - 7 of the tasks are basically solved for both models (> 90%) ==> 16 interesting tasks ~= 32 hours
     - dropping to 100 examples/task might mean only 24 hrs of processing for a run

 - faithfulness measurements
 - thought: book website
 - deep dives

   - multistep_arithmetic_two: 87.9, generator exists
     https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/multistep_arithmetic/task.py

   - geometric shapes: 40 --> 71 after tuning, total of 361 examples in
     https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/geometric_shapes/task.json,
     so 111 more examples

   - logical_deduction_three_objects only has another 62 examples.  total of 72 with 'second' as a substring
     there is also the five-object version though, 500 total and 295 with 'second'.
     have not tested tuning

   - formal fallacies is 83.3 with sonnet 3.5, and there are thousands in BB

   - ruin_names is mid-80's

  bad choices:
  - salient_translation_error_detection is only 73% accurate with sonnet 3.5 (on std cot)

8/3
 - logical_deduction_three_objects
   - baseline: 24/30 tune, 28/30 dev
   - clean: 25/30 tune, 28/30 dev
   - but clean is worse on --test_set, 85.8 vs 87.9 (but that's only 3 examples)

 - geometric_shapes baseline-dpt-dev 23/30 overall 23/26 parsed only one () error
 - using --template_file templates2/predict_output_dyck.txt improves to 25/27 parsed 25/30 overall
 - trying full run ... not looking like a big win though - in fact it's lower(!) 72.5 vs 76.2
   - maybe explicitly reversing the stack would help?
   - _rev variant is not better on dev: 81.5 vs 92.6 with quoted template

 - there is a bug in navigate though
   - original with bug: --hi 30 27/30 90%  --lo 30 29/30 96.7%
   - fixed bug: --hi 30 100% --lo 30 29/30 96.7 %

8/2
 - salient_translation_error_detection
   - cassie's latest: 63.3 --hi 30
   - added rubrics as argument to find_translation_error, choose_error_type
     - similar performance on --hi 30: 66.7 and --lo 30: 66.7
   - ran on test_set, performance is very similar to the cot
   - the PTP model doesn't reflect as much on the differences between
     the translations as the CoT model does, maybe thegre's a chance
     for improvement here?

 - temporal_sequences 93.3 --hi 30
 - the _clean variant of logical_deduction_three_objects gets rid of using ... and Any
   as wildcards, it might be somewhat better than the old version.  I haven't
   tried adding microtraces.  The two errors on dev are about 'second from the end'
   which was the main place where the standard model failed. it scored 28/30 which
   is the same as the standard model.

 - opportunities for tuning are limited
   - arithmetic and geometric_shapes are the only alg tasks with high error
   - nlp tasks with high error rate seem to be mostly noisy
     - ruin_names - noisy?
     - movie_recommendations - noisy?
     - formal fallacies - maybe a different method could be used with
       many microtraces, where you just memorize logical theories
       	 nltk prover: https://www.nltk.org/howto/inference.html, https://www.nltk.org/howto/drt.html
     - salient_translation_error_detection might be the only one with a real upside

7/26
 - have dpt/cot results for all problems
 - temporal_sequences seems wonky and so does geometric_shapes
   - geometric_shapes: dev set is statistically unlikely to be so different from test
   - temporal_sequences: same (although I didn't do a test - Cot makes
     2 errors in dev and 0 in test set)

7/22
 - new formal_fallacies_tuned: just trying to fix to_logical_form
   - baseline: 15/22 and 8 parse failures, or 68.2 ==> tuning *lowers* to 56.6
     - tuning errors include mistaked use of contrapositive
     - working with this logical formulation might just be too hard?
       should I revise the data structure? maybe just use text?
 - dyck_languages: added instructions and examples for extract_dyck_input to make it
   more robust to the extra-close-paren errors.  This got us from 66 to 75.
 - ran UNTUNED dyck_languages on first 150, need to run on the last 40.  About 75% so
   far but I know it ended at 66% so... crap

 - caught model indenting logs for multistep_arithmetic_two - should I encourage this?
   

7/19
 - ran on tune set: logical_deduction_three_objects, dyck_languages, formal_fallacies
 - run Cot on test: formal_fallacies, logical_deduction_three_objects, dyck_languages
 - annotated and entered: formal_fallacies, 
 - cassie annotated logical_deduction_three_objects
 - cassie asked for help on geometric_shapes and dyck_languages
 - did dyck_languages
    - most errors have to do with copying the original call over somehow,
    I'm not totally sure how to fix that.
    - a few have to do with copying in complex outputs
 - need to annotate geometric_shapes still

 - verified again that an error is raised in logutil.py when a syntactically
   function call/return pair for a hallucinated function is produced

7/15

interesting results analyzing date_understanding

 - date_understanding had not been tuned, since it was at 96.7% already.  It did way
   worse on the test set (I think the same is a little non-representative, the
   problems are not really IID)

 - the date_understanding.py program has a bunch of magic constants
   ("today", "answer") which mean that the information flow from
   routine to routine is not based on copying.  looking at the logs on
   tuning data, there were errors I could not localize to a routine,
   instead the errors were control-flow - calling the wrong routine in
   the wrong place (or the right routine with the wrong magic constants).

 - cleaning up the code to date_understanding_v2 helps a lot with
   localization That story about Jane sort of breaks the top-level
   algorithm, but you could say that
   find_time_interval_to_today(question, '12/02/2016') should return
   'three years after one day before' so then you get 12/01/2019 for
   today, and yesterday is 11/30/2019.
 
   'Jane got her job in 2016. Today is her 3-year work
    anniversary. She still remember that on Dec 2, her second day at
    work, she spilled coffee on her laptop. What is the date yesterday
    in MM/DD/YYYY?'


7/12
 - ran boolean_expressions and got a win, about 15-20min per run of 190 examples
 - geometric_shapes - started a PTP run with $55 balance

 - BUG: is running on test clobbering my old logs?
 - BUG: analyze_input in boolean_expressions is typed wrong (!)

 - status of tuned models
   - boolean_expressions_tuned is ok
     - what about the CoT prompt? fixed
     - note: "removing When you give your answer, follow the format of
       the examples below carefully" changes the output quite a bit,
       and eliminates one error.
   - disambiguation_qa is ok
     - what about the CoT prompt - ok
   - geometric_shapes_tuned.py - ok
   - reasoning_about_colored_objects_tuned.py - ok

   cassie still working on?
   - salient_translation_error_detection is ok
     - what about the CoT prompt - ok

 - estimating costs: for boolean_expressions -CoT test --delay 0.5 
   - start approx 72.50 10:20am and 10:32 $71.56 so about 10-15 min and $1.00
   - 23 tasks will be about $25 per condition
   - results: CoT better than expected, 98.4, ptp ties, an extra error or so with tuning.


7/2
 - causal_judgement
  - weird call. tune 3: joe was bad, drunk driver hurt son: yes joe's fault (model says no)
  - good call. tune 5: students cheated, not John, he got the 20th score, someone else didn't. no, not john's fault
  - good call. tune 7: zach followed rules, didn't tell harmon something, harmon out.  zach's fault? no
  - contradicts 15. tune 10: prof smith applied 2x, only needed 1 "yes", both approve,
    dept committee was a surprise.  Did dept committee cause grant success? no 
  - good call. tune 14: john was exposed to cancer at work, not his
    fault, died from something else entirely. was it the job? no
  - contradicts 10. tune 15: bill and su each bought battery, only one
    needed.  was bill responsible for the car starting? yes
    -- between 10 and 15, dept committee was unexpectedl
  - good call. tune 16: like 15, but was bill responsible for a pair? yes
  - good call. tune 19. sniper, heated up a gun (as expected) sniping.  did he intentionally heat the gun? mo
 - my rules
   - reason why doing things matters (19)
   - doing wrong makes it more likely to be your fault (3, 7)
   - multiple causes: unlikely-to-occur extra causes don't count?
 - observation
   - sometimes the system doesn't make an inference for every relevant sentence

 - geometric_shapes 
    - changes:
      - added is_unique_answer(matching_options)
      - added a bunch of summary-matching rules
  - total 1 doesn't know to take an extra step when there are two options
  - total 3 miscounts straight lines in a M x,y L x1,y1 M x1,y1, ... pattern *2
  - total 4 thinks five straight touching lines != pentagon
  - total 6 has three choices and tries them all
  - total 8 miscounts straight lines in a M x,y L x1,y1 M x1,y1, ... pattern
  - total 10 
    - has a rectangle length-cluster and guesses kite
    - does not recognize num_consecutive_touching_lines=4 as a possible rectangle
  - total 12
    - has two answers circle and ellipse, can't choose correctly
  - total 17
    - miscounts #lines (gets 7 vs 8)
  - total 19?
    - guesses num_consecutive_touching_lines=3 is a kite
  - total 20?
    - thinks num_consecutive_touching_lines=6 is a heptagon (=7 sides)
  - total 24
    - rectangle/trapezoid, cannot guess which
  - total 28
    - out of space, doesn't know from octagon
  - total 30
    - doesn't know heptagon

6/28
 - refactored logutil and run_eval2
 - logutil now mostly works as intended
   - found typing errors and similar style bugs in several mocks
 - standardized mock names

6/27
 - tuning reasoning_about_colored_objects
    - starting point => baseline dpt: Final totals correct=24 total=30 acc=0.8 on tune
      first error is around objects with counts! so added counts for each colored object, as in object_counting 
      (really should split query_colored_objects into a string output, and a match to the answer)
    - make reasoning_about_colored_objects.tunedpt-tuned
      => Final totals correct=22 total=30 acc=0.7333333333333333 on tune
    - added 7 examples, all of query_colored_objects
      - comment: query_colored_objects could have been broken into a text response and a best option
      => Final totals correct=27 total=30 acc=0.9 but Final totals (ignoring parse failures) correct=27 parsed=27 acc=1.0
   - running on dev: Final totals correct=19 total=30 acc=0.6333333333333333
     - why is this worse? redoing old baseline on dev (I did change temperature)
       - failed because service is overloaded @ correct=13 total=19 parse_failures=0 prediction='Q' y='(Q)'
         next was correct=5 total=11 => total is 18/30 = 0.60
   - anthropic is being flaky: testing on gemini
   baseline dpt dev => Final totals correct=30 total=30 acc=1.0
   baseline cot dev => was 19/29 with 7 parse failures then quota exhausted, last is correct, free tier is 15 RPM 
    and I was doingt more I guess
     parse failures correct: 1,4,10,17,19,28 = 6 => 25/30 total.

6/26
  - paper outline
  method
  subfunctions:
    - syntactically correct, type-correct, plausible
    - type-correct checking:
      - see colab https://colab.research.google.com/drive/1uPIGNloNSsRKMpaXQijJD3ncptLIA-vR#scrollTo=4oFRn0j2zJxg
      - started hacking logutil to do the work

  - working on tuning causal_judgement (baseline CoT 83.3)
  => baseline program traces 15/30 50.0 but has: 6 refusals: 0.625
  -- changed function names from plausible to reasonable: didn't help (but didn't do it right)
  -- ran on tune dataset: 20/30 correct, 3 are refusals, 0.74
  -- added 4 subfunction examples: 22/30 correct on tuning, 0.81
  -- added 5 more: 23/30 correct on tuning, that's 0.85
  -- on training set
  => Final totals tuned => correct=19 total=30 acc=0.6333333333333333 overall
  => but 6 refusals! so 19/24 = 0.7916666666666666

  - other priorities
    tune logical_deduction_three_objects
    disambiguation_qa

6/24
  - word_sorting_recursive seems worse - 60% instead of 73%
  - making the bucket keys be just letters gets back to 73#
  - trying removing the recursion shortcut I tried... - up to 93

  - formal_fallacies_simplified_compact got 53.3% with 3 parse failures
  => Final totals correct=16 total=30 acc=0.5333333333333333
  => logs2/formal_fallacies/simpcomp-dpt-dev.log

6/23

 - simplified formal_fallacies 12/30 correct and 8 parse failures = 12/22 correct = 54.5%
   - parse failures ALL cutting off too soon
   - simplified on gemini flash gets 50% on dev, no parse failures BUT
     sometimes generates a program instead following directions
   - adding a better prompt reduces this but still at only 12/30 = 0.40
   
 - as a sanity check, I just converted the dev set formula to CNF with a branch of the older
 model and ran it though my resolution refutation code (resref.py here)
 => Final totals: total=30 correct=10 errors={'syntax': 3, 'cnf': 2} ys={'valid': 19, 'invalid': 11}
 => logs2/formal_fallacies/fromcnf-dtp-dev.log, logs2/formal_fallacies/tocnf-dtp-dev.log
 => baseline cot: 17/30 acc = 0.5666666666666667 with manual labeling

 - added word_sorting_simplified_sc.py which (1) introduces errors in
   the cot examples and (2) does self-consistence *within the prompt*
   over 3 trials.  that gets to 90% on dev.

6/21

 - word_sorting_simplified is at: Final totals correct=25 total=30 acc=0.8333333333333334
 model just calls sort_words which calls sorted(words)

 - word_sorting_simplified2 sorts 2x, which introduces some errors, and gets to 90

6/20

 - added word_sorting.  seems to work ok, about as well as the cot
 - prompting, both are about 22/23 of 30.  test: adding an
 - 'explanation' for the sorting, didn't help at all.  A special case
 - to not sort singleton lists also did not help, performance is now worse.

6/19
 
Some comments on reviewing results: https://docs.google.com/spreadsheets/d/1TetyIRjtMzIS-6I3OgmvHfi4sRMgF5pWJL_sOZwlpcM/edit?gid=0#gid=0

 - reasoning_about_colored_objects is not as good as I thought, most
    of the cot errors were failed parses (it's 83 vs 90 for cot).
    problems with doctrace prompt are mostly removals and negations.

    - experiment: just improved the docs for the query subroutine to
      see what would happen.  that didn't help.

    - experiment: added an 'explanation' output to the query subroutine,
      that also make no difference.

 - logical_deduction_three_objects is 76.7 vs 96.7 for CoT.
    Simplifying the code got to 83.3, making the format of constraints
    closer to the cot prompt gets to 90.

logical_deduction_three_objects: 
 => logical_deduction_three_objects/baseline-dtp-dev.log
 => Final totals correct=23 total=30 acc=0.7666666666666667

dyck_languages: need a whole program to work but:
 => ../doctest-prompting-data/logs2/dyck_languages/baseline-dpt-dev.log
 => Final totals correct=24 total=30 acc=0.8

baseline cot:
 => logs2/dyck_languages/baseline-cot-dev.log 
 => Final totals correct=0 total=30 acc=0.0
 Manual labeling (oops missed one): 14/29 = 48%
 ynynnyyyynnnyynnynynynynnnnyy

python3 run_eval2.py --hi 30 dyck_languages | tee 

6/18

reasoning_about_colored_objects - just used a python list of colored
objects and a natural-language query, and it seems to work fine:

reasoning_about_colored_objects - first cut on training data (first
30) with first working version is:

 with program trace prompting
 => logs2/reasoning_about_colored_objects/baseline-dpt-train.log
 => Final totals correct=25 total=30 acc=0.8333333333333334

 For cot baseline:
 => logs2/reasoning_about_colored_objects/baseline-cot-train.log
 => Final totals correct=17 total=30 acc=0.5666666666666667

Formal fallacies saga:

 * newer version of formal_fallacies gets 10/23 correct on dev before
failing with gemini error.  But also there are 4 parse-failed because
the output was too long, so 10/19 for the cases that didn't happen.
So 52.6%.  Another 43 cases where output is > 4096.

 * probably worth seeing how often handoff to a real theorem prover
works?

 * could also try converting to CNF, then producing the cot reasoning
 steps as resolutions.  could also restrict to proving implications
 only.

 suggest_next_inference(cnf) -> Step(i, j, lit)
 apply_inference(cnf, step) -> cnf
 hypothesis_is_valid(cnf) -> bool, str | None, 'cannot tell yet'

6/13

reasoning_about_colored_objects - questions are all a little different
in the cot examples.  roughly, the task is to visualize/parse the
input (an ordered list of named objects with colors) and then execute
a query, but the query language is unclear from the cot examples

penguins_in_a_table - questions are all different in the cot examples.
roughly, the task is to visualize/parse the input (a table of named
penguins with a few attributes) and then execute a query, which
involves sorting one column of the table and selecting the nth

object_counting - looks easy

logical_deduction_three_objects - I think this can be done with
resolution and a transitive rule.

Q1: [eve above amy][eli below amy]
    less(amy, eve) less(eli, amy) => less(eli, eve)
    (A) last(amy) = [not less(eli,amy)], [not less(eve, amy)] -X
    (B) last(eli) = [not less(amy, eli)], [not less(eve, eli)] -check
    (C) last(eve) = [not less(amy, eve)], [not less(eli, eve)] -X
Q2: [green right of white] [orange rightmost]
    less(white, green) less(green, orange), less(white, orange)
    (A) first(white) = less(white, green), less(white, orange) -check
    (B) first(green) = ...
    (C) ...
Q3: [white left of gray][red second from left]
    less(white, gray)
    red 2nd from left  = [less(white, red), less(gray, red)][less(red,gray), less(red, white)]
    ...

formal_fallacies - in 50's for models in the chain of code paper -
seems doable with resolution.

Q1: S1 c1.1 ["Lesley is a close friend of Fernando"]
    S2 c2.1 ["X is not close friend of Fernando", "X is great-grandfather of Leroy"]
       c2.2 ["X is not schoolmate of Lowell",  "X is great-grandfather of Leroy"]
    q: "Lesley great-grandfather of Leroy"?
    Proof: c1.1 + c2.1 => ["Lesley is great-grandfather of Leroy"] = q so q follows

Q2: S1 c1 ["X great-grandfather of Clyde", "X stepbrother of Brian"]
    S2 c2 ["not X ancestor of Dana", "not X a great-grandfather of Clyde"]
    q: ["not X is ancestor of Dana", "X stepbrother of Brian"]?
    proof: c1 + c2 => ["X stepbrother of Brian","not X ancestor of Dana"] = q so q follows

Q3: S1 c1 ["not X infrequent user of Paul Mitchell shampoo", "X rare consumer of Nioxin shampoo", "X loyal buyer of Caress soap"]
    S2 c2 ["not X regular consumer of Lush soap", "not X rare consumer of Nioxin shampoo", "not X loyal buyer of Caress soap"]
    q: ["not infrequent user of Paul Mitchell shampoo", "not X regular consumer of Lush soap"]?
    proof: c1 + c2 => tautology so q does not follow

Verified hyperbaton, sports_understanding are 60 examples in train
 - Ran dev sets
 ==> sports_understanding/baseline-cot-dev.log: Final totals correct=0 total=30 acc=0.0
 Manual tagging, since the prompts don't force A/B: bggggggbggggggbggggggggggggggg: correct=28/30 = 93.333
 ==> sports_understanding/baseline-dpt-dev.log: Final totals correct=28 total=30 acc=0.9333333333333333
 ==> hyperbaton/baseline-cot-dev.log: Final totals correct=25 total=30 acc=0.8333333333333334
 ==> hyperbaton/baseline-dpt-dev.log: Final totals correct=30 total=30 acc=1.0

Locally on my mac:
 - baseline CoT on dev set (--lo 30) 
   => logs2/ruin_names/baseline-cot-dev.log
   => Final totals correct=21 total=30 acc=0.7
 - baseline doctest prompt (dpt) on dev set
   => logs2/ruin_names/baseline-dtp-dev.log
   => Final totals correct=22 total=30 acc=0.7333333333333333
 - baseline doctest prompt (dpt) on train set
   => logs2/ruin_names/baseline-dtp-train.log
   => Final totals correct=22 total=30 acc=0.7333333333333333
 - scraped logs with python3 retrain_ruin_names.py ==> mocks2/ruin_names_training_updates.json 
 - updated the ruin_names.py code -> mocks2/ruin_names_update.py
 - convered to a prompt python3 ruin_names_update.py --partialprogram_name ruin_names_update.py partialprogram
 - moved ruin_names_update.py to mocks2/partialprograms-updated
 - ran result on dev again
   => logs2/ruin_names/retrained-dtp-dev.log
   => Final totals correct=23 total=30 acc=0.7666666666666667

      	    cot	   dtp	  dtp++
ruin_names  21	   22	  23
hyperbaton  25	   30	  -
sports	    28	   28	  -


6/12
 - only 60 examples of train/ruin_names (!)
 - proposal: use last 30 as dev set, first 30 as train

 - Baseline: cleaned up ruin_names.py program with no
   subroutine-mocked doctests at all on the dev set
   => Final totals correct=20 total=30 acc=0.6666666666666666
   => ruin_names-dev-baseline.log

 - Next step: ran baseline on first 10 training (--hi 10)
   => ../doctest-prompting-data/subroutine-example-logs/ruin_names-dev-baseline.log
   Extracted examples from the these (only a couple!) and updated ruin_names.py prompt
   Ran the resulting model on dev again
   => Final totals correct=23 total=30 acc=0.7666666666666667
   => ../doctest-prompting-data/logs2/ruin_names-dev-first10.log

 - Next step: ran the first10-augmented baseline on the second 10 training examples
   => ../doctest-prompting-data/subroutine-example-logs/ruin_names-second10-first10.log 
   => Final totals correct=10 total=10 acc=1.0
   So no examples that can help! So I went to the third minibatch of 10 training examples
   ../doctest-prompting-data/subroutine-example-logs/ruin_names-third10-first10.log  - Next step:

   Spliced that data in and rebuilt ruin_names.py
   re-ran the dev tests => ../doctest-prompting-data/logs2/ruin_names-dev-first30.log 
   => Final totals correct=24 total=30 acc=0.8

 - Just for fun, also ran on the 30 training data cases
   => Final totals correct=24 total=30 acc=0.8
   => ../doctest-prompting-data/logs2/ruin_names-first30-first30.log 
   So there are still lots of training examples to collect
   - looking over it, I see two examples ruining 'the police' ???

 see doc for cassie on snarks
  - analyze_input(input_str): choices [('A', statement_A), ...]
  - summarize_statement(statement): produce the 'it says that' version of the sentence
  - judge_summary(summary)

6/11
 - updated with simpler mocks2
 sports_understanding: Final totals correct=11 total=12 acc=0.9166666666666666
 hyperbaton: Final totals correct=12 total=12 acc=1.0
 ruin_names: Final totals correct=9 total=12 acc=0.75

6/4
 - acronym: CoSMOP Chain of Subroutine-MOcked Program
 - Precision Prompting with Partial Programs

 - thoughts on current setup
   - could have arbitrary ###START_HIDDEN [...] ###END_HIDDEN markers
   - could have general ###INSERT_TRACES fn_name - if there is a general INPUTS[fn_name] dictionary
      - could also have a COMMENT[fn_name][input_str] for pre-example comments.
   - coult have a wrapper decorator that traces inputs/outputs of functions
      - that would simplify the tracing I guess, but would also add printf's in
      	any mock traces.  And the program gets more complex.

6/3

 - sports_understanding: Final totals correct=12 total=12 acc=1.0
 - ruin_names: Final totals correct=10 total=12 acc=0.8333333333333334
 - hyperbaton: took about 90min to get a test working, initial test is 8/12 correct
   -- all the failures come from the anthropic model failing to follow instructions,
      not simulating an output, which leads to a **parse failed** prediction.
   -- fixing this in the prompt got me to Final totals correct=12 total=12 acc=1.0
      QUESTION: Below, predict what the output be of the program be, given
      the input below.  Respond with ONLY the expected program output - you
      will be penalized if you introduced any additional explanatory text.

 Round two: adding the new prompt perturbed everything, tweaked things a bit more
 logs/hyperbaton-eval.log    	    Final totals correct=12 total=12 acc=1.0
 logs/long-ruin_names-eval.log 	    Final totals correct=44 total=60 acc=0.7333333333333333
 logs/ruin_names-eval.log 	    Final totals correct=8 total=12 acc=0.6666666666666666
 logs/sports_understanding-eval.log Final totals correct=12 total=12 acc=1.0


5/31
 - exploring datasets
   - chain-of-code has nothing, just a colab with some demos
   - new peter clark is a random collection of benchmarks
   - lookingh CoT results from chain-of-code what are the hard tasks?
     - sports_understanding is easy: cot=96, interweave=91
     - some interesting ones

  -tip: docstrings with backslashes need to start with r"""

domain 	    		cot		CoCode
causal_judgement	64		56	- ?? the cot examples are terrible
date_understanding	84		75	- with cassie
hyperbaton		64		98	- done
logical deduction	66		68	- ordering 3,5,7 objects
multstep arith		48		100	- arithmetic expressions
ruin names		82		90	- done
snarks			71		76	- sarcasm -done
sports_understanding	96		91	- easy -done
word sorting		50		99


5/30
 - just continuing the input doesn't work well
 - continue_prompt.txt with sonnet for 9/10 correct - really 10/10, one is mislabeled (Malcolm Brogdon)
 - continue_prompt.txt with haiku was all over the map, got 2/10 correct, mostly did not answer the question
 - simulate_prompt.txt with haiku got 8/10 correct.  One error was based on misclassifying a soccer player as a basketball player
   - after adding ```brackets around the program``` got 17/20 correct with haiku

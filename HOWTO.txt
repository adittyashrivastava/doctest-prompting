Installation: To get a setup like mine:

   set up a data and/or working directory: mine is ~/code/doctest-prompting-data/

   in the data directory create these empty subdirectories 
	  doctest-prompting-data/logs2
   	  doctest-prompting-data/special-logs

   in another directory, git clone https://github.com/wwcohen/doctest-prompting.git
   (this is my code)

   cd to doctest-prompting/mocks2 and make subdirectories 'doctests' and 'partialprograms'
   

Code: mostly intended to be run from the code directory.  You will need also to add the
   main code directory to your PYTHONPATH for some things, and any API keys will
   need to be set in your environment.

   analysis_scripts/
     - scripts I used for experiments in the paper, plus a few others I didn't end of using

   examples/ holds the train/test split of the examples.  The 'dev' split
   is the first 30 in train and the 'tune' split is the last 30 in train.

   mocks2/ directory of task mocks, also contains example mocks, 'partialprograms' (which 
   are the programs included in a prompt) and 'doctests' 

   modified-cot-prompts/ - the CoT prompts we use as baselines

   synthetic/ - synthetic task generation (not being used now)

   argutil.py - utils for python command-line args

   debugger.py - utils for executing single-step prompts.  There is
   also a main that allows use to do this interactively.

   example_util.py - a main, for viewing examples in non-json formats
   and for splitting the original examples into train/test.  You probably
   won't need this.

   expt_util.py - a main for doing modularity experiments

   hows_it_going.py - a main for checking results of a long-running run-eval.py job

   llm_util.py - code for accessing various LLMs

   local_model_util.py - for experiments on the Babbel cluster
   job_util.py - for experiments on the Babbel cluster

   log_util.py - for analysis of output logs (from a run_eval2.py job).
   This has a main which you probably won't use.  
    - This probably needs the subdirectory tmp_stubs to exist

   mock.py - used for writing task mocks.

   modularity_expt.py - a main that runs a lot of expt_util.py experiments

   oracle_util.py - a main for running experiments comparing
   single-step execution with an oracle.

   pairwise_log_cmp.py - for doing pairwise comparisons of performance based on logs
   threeway_log_cmp.py - for doing 3-way comparisons of performance based on logs

   psoracle_expt.py - don't use this

   run_eval2.py
      - a main, lets you run some set of contiguous training examples using prompts

   SOME USEFUL OPTIONS FOR run_eval2.py
   for the BBH tasks, 
     "python run_eval2.py --hi 30 TASK" runs on the dev set
     "python run_eval2.py --lo 30 TASK" runs on the tune set
     "python run_eval2.py --lo 30 --test_set" runs on the test set
     --CoT overrides the usual PTP prompt and runs a vanilla chain-of-thought prompt
     --example_dir XXXX over-rides the usual place for examples
     --template_file XXXX over-rides the usual place for a chain of thought prompt



==============================================================================
ABOUT MOCKS

Mocks: these are all named after a BBH task, like
sports_understanding.  They are usually saved in the subdirectoryy
doctest-prompting/mocks2.

  - Some things to do after you cd to mocks2

  % python3 sports_understanding.py pass
  
  checks the syntax of sports_understanding.py

  % python3 -i sports_understanding.py pass

  same, but leaves you in an interactive python, so
  you can debug the code

  % python3 sports_understanding.py check

  checks the preprocessing commands like ###IF prompt  ###ENDIF prompt

  % python3 sports_understanding.py prompt

  creates the 'partial program' (prompt version) of sports_understanding.py
  and stores it in mocks2/partialprograms/sports_understanding.py.
  Only AFTER you do this can you use 'run_eval2' to test your 
  mock out on data.  Specifically calling this does the following

    1. It calls sports_understanding(x) on each x in INPUTS, and saves
    all the print output, and uses that output to create a trace
    of the policy programs output on each input x.

    2. It makes a version of sports_understanding.py in the directory
    templates/ which works as an LLM prompt, specifically

      (a) everything before the ###START_PROGRAM marker and everything
    after the ###END_PROGRAM is removed
    
      (b) everything between a ###START_CODE / ###END_CODE pair is
    removed and replaced with '...'

      (c) some more text is added before/after the program to complete
    the prompt (really a template for the prompt, which needs to have
    the input_str variable filled in.)

  % python3 sports_understanding doctests

  should create and run the doctests for sports_understanding.
  (10/4/24 - currently there is a bug with this)


==============================================================================
HOWTO do Data labeling

 - Get a tuning log (--lo 30) up in a text editor
 - Open the form: https://docs.google.com/forms/d/e/1FAIpQLSf6er4dM4XN67ZFhmv0hzbo8f-BjTb0L8GxSi67HOafRgSgYw/viewform?vc=0&c=0&w=1&flr=0
 - For each example with an error
   - Answer the questions in the form.
     - If there is a localizable error, and we're going to add micro-examples to tune the model, then
       go ahead and add the fixed example.  Otherwise, skip that step.
   - Submit the form

 - Run 'py logutil.py TASK --lo 30'
   - record the number of tests and errors in the sheet (Faithfulness - Sonnet 3)
     https://docs.google.com/spreadsheets/d/1TetyIRjtMzIS-6I3OgmvHfi4sRMgF5pWJL_sOZwlpcM/edit?gid=454032197#gid=454032197
   - if something seems bizarre investigate (eg, non-localizable errors are probably worth diving into)
   - else:
      - record the number of total mistakes on the tuning set and the number that could be localized
      - cp the annotated log into the annotated-logs/SERVICE-MODEL/TASK/
      - update your git repo as needed


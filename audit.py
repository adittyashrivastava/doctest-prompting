"""V2 of process for auditing traces.
"""

import argparse
from icecream import ic

import pandas as pd
import re
import sys

from contextlib import contextmanager
from dataclasses import dataclass, field
from itertools import chain
from scipy.stats import binomtest
from typing import Optional


import log_util

@dataclass
class Trace:
    """Preprocessed trace.
    """
    # original info passed to the trace
    trace_lines: list[str]
    is_correct: Optional[bool] = None
    index: int = -1

    # options for preprocessing
    # for each step evaluate str_inputs str_output
    do_eval: bool = True
    robust_eval: bool = True
    do_combine_multiline_steps: bool = True
    do_expand_compressed_trace: bool = False

    # extracted information
    final_answer: str = field(default_factory=str)
    # steps as dictionaries
    steps: list[dict[str,...]] = field(default_factory=list)
    # errors thrown in preprocessing
    errors: list[dict[str,...]] = field(default_factory=list)

    ENTER_STEP_REGEX = r'\s*Calling (\w+)(\(.*\))\.\.\.\s*'
    EXIT_STEP_REGEX = r'\s*\.\.\.(\w+) returned (.+)\s*'
    FINAL_ANSWER_REGEX = r'\s*Final answer: (.+)\s*'
    ASSIGNMENT_REGEX = r'\s*(v\d+) = (.*)$'

    def __post_init__(self):
      self.steps = []
      self.errors = []
      self.final_answer = '**parse failed**'

      # preprocessing pipeline
      if self.do_combine_multiline_steps:
          self.combine_multiline_steps()
      if self.do_expand_compressed_trace:
          self.expand_compressed_trace()
      self.build_steps()
      if self.do_eval:
          self.eval_steps()

      self.steps_df = pd.DataFrame(self.steps)

    
    def expand_compressed_trace(self):
        """Preprocessing to expand trace produced by cmock.
        """
        trace_lines = self.trace_lines
        expanded_lines = []
        binding = {}
        for line in trace_lines:
            m = re.match(Trace.ASSIGNMENT_REGEX, line)
            if m:
                binding[m.group(1)] = m.group(2)
        for line_num, line in enumerate(trace_lines):
            m1 = re.match(Trace.ENTER_STEP_REGEX, line) 
            m2 = re.match(Trace.EXIT_STEP_REGEX, line)
            m3 = re.match(Trace.ASSIGNMENT_REGEX, line)
            if m1 or m2:
                vars = re.findall('(v\d+)', line)
                for v in vars:
                    if v not in binding:
                        self.errors.append(dict(
                            msg='unbound variable',
                            line=line_num,
                            var_name=v))
                        binding[v] = f'"**unbound variable {v}**"'
                    line = line.replace(v, binding[v])
            if not m3:
                expanded_lines.append(line)
        self.trace_lines = expanded_lines

    def combine_multiline_steps(self):
        """Preprocessing to collapse calls/returns that span multiple lines.
        """
        trace_lines = self.trace_lines
        clean_lines = []
        i = 0
        while i < len(trace_lines):
            buffer = trace_lines[i]
            m = re.match('\s*Calling \w+\(', buffer)
            if m is not None:
                while i + 1 < len(trace_lines):
                    # see if the next line closes this step or starts another
                    m_exit = re.match(Trace.EXIT_STEP_REGEX, trace_lines[i + 1])
                    m_enter = re.match(Trace.ENTER_STEP_REGEX, trace_lines[i + 1])
                    # if not, add this to the current line
                    if not m_exit and not m_enter:
                        buffer = buffer.rstrip() + ' ' + trace_lines[i + 1]
                        i += 1
                    else:
                        break
            clean_lines.append(buffer)
            i += 1
        self.trace_lines = clean_lines

    def eval_steps(self):
        """Add new key/values for evaluated versions in str_inputs and str_output.
        """
        for step in self.steps:
            for key, kind, line_key, is_input in [
                    ('str_inputs', 'input', 'start_line', True),
                    ('str_output', 'output', 'end_line', False)]:
                if key not in step:
                    self.append_preprocessing_error(step, line_key, f'{kind} not in step')
                    continue
                value, had_error = log_util.safe_eval(
                    step[key], is_input=is_input)
                if self.robust_eval and had_error:
                    # try again without single quotes inside the string, a
                    # common problem
                    to_eval = re.sub(r"(\w)'(\w)", r"\1\\'\2", step[key])
                    value, had_error = log_util.safe_eval(
                        to_eval, is_input=is_input)
                if had_error:
                    self.append_preprocessing_error(step, line_key, f'{kind} is not evaluable')
                    continue
                self._insert_evals(step, kind, value, is_input)
        
    def append_preprocessing_error(self, step, line_key, msg):
        """Save info for an error that happened during preprocessing.
        """
        err = dict(
            msg=msg,
            step_fn=step.get('step_fn', '??'),
            line=step.get(line_key, -1))
        # if we don't have a line number then save the whole step for
        # debugging
        if line_key not in step:
            err.update(step=step)
        self.errors.append(err)

    def _insert_evals(self, step, kind, value, pluralize):
        """Insert new step keys for 'value' resulting from eval of
        str_inputs or str_output.
        """
        # store basic eval result as 'inputs' (plur) or 'output' (not plur)
        plur = 's' if pluralize else ''
        step[f'{kind}{plur}'] = value
        # if a tuple store components as foo1, foo2, ...
        if isinstance(value, tuple):
            step[f'len_{kind}'] = len(value)
            for i, vali in enumerate(value):
                step[f'{kind}{i+1}'] = vali


    def build_steps(self):
        """Parse the trace_lines into a list of 'step' dictionaries.

        Keys include step_fn, str_inputs, str_output, start_line, end_line. 
        """
        stack = []
        for line_num, line in enumerate(self.trace_lines):
            m_enter = re.match(Trace.ENTER_STEP_REGEX, line)
            m_exit = re.match(Trace.EXIT_STEP_REGEX, line)
            m_final = re.match(Trace.FINAL_ANSWER_REGEX, line)
            if m_final:
                # record the final answer when we see it
                self.final_answer = m_final.group(1)
            if m_enter:
                # push a new step dictionary onto the stack
                step_fn = m_enter.group(1)
                str_inputs = m_enter.group(2)
                start_line = line_num
                stack.append(dict(
                    step_fn=step_fn, str_inputs=str_inputs, start_line=start_line))
            if m_exit:
                # create a new step dictionary
                step_fn = m_exit.group(1)
                str_output = m_exit.group(2)
                end_line = line_num
                close_step = dict(
                    step_fn=step_fn, str_output=str_output, end_line=end_line)
                # look for matching dict, ideally on stack
                open_step = stack.pop() if stack else dict(step_fn=step_fn)
                if open_step['step_fn'] == step_fn:
                    # save the combined the close and open steps
                    close_step.update(open_step)
                    self.steps.append(close_step)
                else:
                    # with a mismatched open and close, save both of
                    # the mismatched partial steps and record an error
                    self.steps.append(open_step)
                    self.steps.append(close_step)
                    self.errors.append(
                        dict(msg=f'mismatched call and return step_fns ',
                             line=line))
        # if there are any unmatched steps on the stack,
        # save them and record an error
        for step in stack:
            self.errors.append(dict(
                msg=f'call of step without return',
                line = line_num,
                step=step,
                step_fn=step.get('step_fn', '???')))
            self.steps.append(step)

@dataclass 
class Auditor:
    """Subclass this to create a task-specific auditor.

    Designed to look like unittest.TestCase instances.
    """
    trace: Trace
    audits: list[dict[str,...]] = field(default_factory=dict)
    context: list[str] = field(default_factory=dict)

    def __post_init__(self):
        self.df = pd.DataFrame(self.trace.steps)
        self.audits = []
        self.context = [self.__class__.__name__]

    @contextmanager
    def subTest(self, msg, **params):
        self.context.append(msg)
        yield
        self.context.pop()
    
    def assertTrue(self, expr, msg):
        """Confirm that the condition holds, and record the message as
        a completed audit in self.audits.
        """
        augmented_msg = '.'.join(self.context + [msg])
        self.audits.append(dict(
            msg=msg,
            context='.'.join(self.context),
            passed=int(expr)))

    def run_audits(self):
        """Run all audit methods for the trace.

        Audit methods start with 'test_' and include 'assertTrue'
        calls.
        """
        for method_name, method in self.__class__.__dict__.items():
            if method_name.startswith('test_'):
                try:
                    method(self)
                except Exception as ex:
                    print(repr(ex))
                    self.assertTrue(
                        msg=f'exception in {method_name}: {ex}',
                        expr=False)
        # save some stats
        self.failures = ' // '.join(set([a['msg'] for a in self.audits if not a['passed']]))
        self.num_errors = len(self.trace.errors)
        self.is_correct = self.trace.is_correct
    
class NullAuditor(Auditor):
    """A useful default auditer which does nothing.
    """
    def test_null(self):
        self.assertTrue(msg='null auditer', expr=True)

class Runner:

    def __init__(self, examples, auditer_class, **trace_kw):
        self.examples = examples
        self.auditer_class = auditer_class
        self.trace_kw = trace_kw
        self.results = []

    @staticmethod
    def from_logfile(log_filename, auditer_class, **trace_kw):
        examples = log_util.load_examples(log_filename)
        return Runner(examples, auditer_class, **trace_kw)

    def run(self):
        audits = []
        for i, ex in enumerate(self.examples):
            trace = Trace(
                trace_lines=ex['output'], 
                is_correct=ex.get('is_correct'),
                index=i,
                **self.trace_kw)
            a = self.auditer_class(trace)
            a.run_audits()
            self.results.append(a)

    def report(self, verbose=0):
        print('examples:', len(self.examples))
        print('results: ', len(self.results))
        audits_df = pd.DataFrame(chain(*[result.audits for result in self.results]))
        print('audits:  ', len(audits_df))
        print('passed:  ', audits_df.passed.mean())
        result_df = pd.DataFrame([
            dict(failures=r.failures,
                 num_errors=r.num_errors,
                 is_correct=int(r.is_correct))
            for r in self.results])
        acc = result_df.is_correct.mean()
        print('accuracy:', acc)

        if not verbose: 
            return audits_df, result_df

        rows = []

        def add_binom_test(rows, tag, tag_df):
            n = len(tag_df)
            if n > 0:
                k = tag_df.is_correct.sum()
                pval=binomtest(n=n, k=k, p=acc).pvalue
                rows.append(dict(
                    failed_audit=tag,
                    k=k, n=n, acc=k/n, pval=pval))

        for msg in set(audits_df.msg):
            msg_failure_df = result_df[result_df.failures.str.contains(msg)]
            add_binom_test(rows, msg, msg_failure_df)

        # row for no audits failed 
        no_failure_df = result_df[result_df.failures == '']
        add_binom_test(rows, 'no audits failed', no_failure_df)

        # row for any audits fail
        no_failure_df = result_df[result_df.failures != '']
        add_binom_test(rows, 'any audit failed', no_failure_df)

        # row for any error
        error_df = result_df[result_df.num_errors > 0]
        add_binom_test(rows, 'any error', error_df)

        # row for no errors
        error_df = result_df[result_df.num_errors == 0]
        add_binom_test(rows, 'no errors', error_df)

        by_failure_df = pd.DataFrame(rows)
        print()
        print('results by individual failing audits')
        print(by_failure_df.sort_values(by='acc'))

        return audits_df, result_df, by_failure_df

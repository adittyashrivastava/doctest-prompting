import collections
from contextlib import contextmanager
import dataclasses
import pandas as pd
import pprint
import re
import unittest

from scipy.stats import binomtest
from scipy.stats import fisher_exact

import arg_util
import debugger
import log_util

class ParsedTrace:
    """A parsed PTP trace.

    Attributes are final_answer and df, where df is a Pandas DataFrame
    with keys 
     - fn_name (the 'function name' of the step
     - inputs (a tuple of input values)
     - output (the single output of a step),
     - start_line, end_line (the position of the step in the trace)
    """

    def __init__(self, logged_trace, robust_parse=True, eval_args=True, eval_return_value=True):
        self.final_answer = None
        self.robust_parse = robust_parse
        self.eval_args = eval_args
        self.eval_return_value = eval_return_value
        for line in logged_trace:
            if line.startswith('Final answer: '):
                self.final_answer = line[len('Final answer: '):].rstrip()
        if self.robust_parse:
            logged_trace = self.cleanup(logged_trace)
        step_outputs = [
            so 
            for so in debugger.outputs_of(None, logged_trace, raise_error=True)]
        def get_args(so):
            return self.eval_step_args(so) if self.eval_args else so.args
        def get_return_value(so):
            return self.eval_step_return_value(so) if self.eval_return_value else so.args
        self.df = pd.DataFrame(
            dict(inputs=[get_args(so) for so in step_outputs],
                 output=[get_return_value(so) for so in step_outputs],
                 fn_name=[so.fn_name for so in step_outputs],
                 start_line=[so.start_line for so in step_outputs],
                 end_line=[so.end_line for so in step_outputs])
            )

    def robust_eval(self, so, to_eval, is_input):
        result, had_error = log_util.safe_eval(to_eval, is_input=is_input)
        if had_error:
            if self.robust_parse:
                # try again without single quotes inside the string, a
                # common problem
                to_eval = re.sub("(\w)'(\w)", "\1\\'\2", to_eval)
                result, had_error = log_util.safe_eval(to_eval, is_input=is_input)
            if had_error:
                where = {True:'args', False:'return'}
                raise ValueError(f'eval error for {where[is_input]} line {so.start_line}: <{so.args[0:340]}>')
        return result

    def eval_step_args(self, so):
        return self.robust_eval(so, so.args, is_input=True)

    def eval_step_return_value(self, so):
        return self.robust_eval(so, so.value, is_input=False)

    def cleanup(self, logged_trace):
        """Collapse step calls/returns that span multiple lines.
        """
        clean_trace = []
        i = 0
        while i < len(logged_trace):
            buffer = logged_trace[i]
            m = re.match('^Calling (\w+)\(', buffer)
            if m is not None:
                called_fn = m.group(1)
                while (i + 1 < len(logged_trace)
                       and not logged_trace[i + 1].startswith(f'...{called_fn} returned ')):
                    buffer = buffer.rstrip() + ' ' + logged_trace[i + 1]
                    i += 1
            clean_trace.append(buffer)
            i += 1
        return clean_trace

    def flat_input_set(self, step_name):
        """A set derived from a flattened list of all inputs to steps
        with the given name.
        """
        return set([inp
                    for step in self.df[self.df.fn_name == step_name].itertuples()
                    for inp in step.inputs])

    def output_set(self, step_name):
        """A set derived from a list of all outputs to steps with the
        given name.
        """
        return set([step.output
                    for step in self.df[self.df.fn_name == step_name].itertuples()])

    def flat_output_list(self, step_name):
        """A flattened list of all outputs to steps with the given
        name.

        This will only work for steps that return tuples as outputs.

        """
        return [outp 
                for step in self.df[self.df.fn_name==step_name].itertuples()
                for outp in step.output]

class Auditor:
    """Subclass this to create a task-specific auditor.

    Designed to look like unittest.TestCase instances.

    Attributes are
    - audits: list of audit messages for audits that have been run
    - failures: list of messages for failed audits
    - parse_error: None or an Exception.  If an Exception then no audits will be run.
    - index, logged_trace: the trace that was audited and its index in the log file.
    """

    def __init__(self, logged_trace, index, **kw):
        self.sub_audits = [self.__class__.__name__]
        self.audits = []
        self.failures = []
        self.index = index
        self.logged_trace = logged_trace
        try:
            self.trace = ParsedTrace(logged_trace, **kw)
            self.parse_error = None
        except ValueError as ex:
            self.trace = None
            self.parse_error = ex

    @contextmanager
    def subTest(self, msg, **params):
        self.sub_audits.append(msg)
        yield
        self.sub_audits.pop()

    def assertTrue(self, expr, msg):
        """Confirm that the condition holds, and record the message as
        a completed audit in self.audits.

        If the condition is false, also record the msg as a failed audit
        is self.failures.
        """
        augmented_msg = '.'.join(self.sub_audits + [msg])
        self.audits.append(augmented_msg)
        if not expr:
            self.failures.append(augmented_msg)

    def run_audit(self):
        """Run all audit methods for the trace.  Audit methods are
        methods starting with 'audit_'.
        """
        if self.trace is not None:
            for method_name, method in self.__class__.__dict__.items():
                if method_name.startswith('test_'):
                    method(self)

    def __str__(self):
        return f'audits: {len(self.audits)} failed: {len(self.failures)} syntax errors: {self.parse_error!r}'

#
# task specific auditors
#

class AuditMedcalcRules(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)

    def test_analyze_input(self):
        trace = self.trace
        self.assertTrue(
            msg=f'one step with fn_name of "analyze_input"',
            expr=(len(trace.df[trace.df.fn_name == 'analyze_input']) == 1))
        
    def test_each_rule_applied(self):
        trace = self.trace
        question, rules, patient_note = trace.flat_output_list('analyze_input')
        num_rules = len(rules)
        for fn_name in ['get_data', 'convert_units', 'check_rule', 'accumulate_score']:
            self.assertTrue(
                msg=f'one step per rule with fn_name of "{fn_name}"',
                expr=(len(trace.df[trace.df.fn_name == fn_name]) == num_rules))
            

class AuditGeometricShapes(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)

    def test_single_steps(self):
        trace = self.trace
        # each of the extract_path, decompose_path,
        # summarize_decomposed_path, and extract_options steps are
        # only used once
        for fn in ['extract_path', 'decompose_path',
                   'summarize_decomposed_path', 'extract_options']:
            self.assertTrue(
                msg=f'one step with fn_name of "{fn}"',
                expr=(len(trace.df[trace.df.fn_name == fn]) == 1))
    
    def test_option_matching(self):
        trace = self.trace
        df = trace.df
        # the number of extracted options equals the number of
        # 'summary_matches_option' steps in the trace
        extract_options_step, *_ = df[df.fn_name == 'extract_options'].itertuples()
        options = extract_options_step.output
        self.assertTrue(
            msg=(f'the number of options ({len(options)}) equals the number of ' +
                 f'summary_matches_option steps'),
            expr=(len(df[df.fn_name=='summary_matches_option']) == len(options)))

    def test_length_clusters(self):
        trace = self.trace
        df = trace.df
        matching_option_outputs = df[df.fn_name=='summary_matches_option'].output.tolist()
        ctr = collections.Counter(matching_option_outputs)
        # if only one summary_matches_option step returns True, then
        # there should be no calls to the compute_length_clusters or
        # relate_length_clusters_to_option steps
        if ctr[True] == 1:
            with self.subTest(msg='one matching option'):
                self.assertTrue(
                    msg='no compute_length_clusters steps',
                    expr=(len(df[df.fn_name=='compute_length_clusters']) == 0))
                self.assertTrue(
                    msg='no relate_length_clusters_to_option steps',
                    expr=(len(df[df.fn_name=='relate_length_clusters_to_option']) == 0))
        # if more than one summary_matches_option step returns True,
        # then there should be one calls to the compute_length_clusters,
        # and one call to relate_length_clusters_to_option for each
        # option for which summary_matches_option returned True
        if ctr[True] > 1:
            with self.subTest(msg='multiple matching options'):
                self.assertTrue(
                    msg='one compute_length_clusters step',
                    expr=(len(df[df.fn_name=='compute_length_clusters']) == 1))
                # model should then check each option against the length clusters
                matching_option_inputs = df[df.fn_name=='summary_matches_option'][df.output == True].inputs.tolist()
                matching_options = [inp[1] for inp in matching_option_inputs]
                self.assertTrue(
                    msg=(f'the number of matching options ({len(matching_options)}) equals the number of ' +
                         f'relate_length_clusters_to_option steps'),
                    expr=(len(matching_options) == len(df[df.fn_name=='relate_length_clusters_to_option'])))


class AuditSportsUnderstanding(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)

    def test_analyze_sentence_step(self):
        # there should be one analyze_sentence step, which returns
        # either 2 or 3 non-empty-string values
        trace = self.trace
        df = trace.df
        self.assertTrue(
            msg='there is one step with fn_name of "analyze_sentence"',
            expr=(len(df[df.fn_name=='analyze_sentence']) == 1))
        analyze_sentence_outputs = [
            outp for outp in set(trace.flat_output_list('analyze_sentence')) if outp != '']
        self.assertTrue(
            msg='the analyze_sentence step has 2-3 outputs',
            expr=(2 <= len(analyze_sentence_outputs) <= 3))

    def test_sport_for_inputs(self):
        # all inputs to sport_for are outputs of analyze_sentence
        trace = self.trace
        analyze_sentence_outputs = set(trace.flat_output_list('analyze_sentence'))
        sport_for_inputs = trace.flat_input_set('sport_for')
        self.assertTrue(
            msg=f'every input to sport_for output of analyze_sentence',
            expr=(sport_for_inputs <= analyze_sentence_outputs))

    def test_consistent_sport_inputs(self):
        # all inputs to consistent_sports are outputs of sport_for
        trace = self.trace
        sport_for_outputs = trace.output_set('sport_for')
        consistent_sports_inputs = trace.flat_input_set('consistent_sports')
        self.assertTrue(
            msg=f'every input to consistent_sports output of sport_for',
            expr=(consistent_sports_inputs <= sport_for_outputs))

    def test_answer_no(self):
        # if final answer is not 'yes' then some call to
        # consistent_sports returns False
        trace = self.trace
        if trace.final_answer.lower() != 'yes':
            with self.subTest(msg='answer is no'):
                consistent_sports_outputs = trace.output_set('consistent_sports')
                self.assertTrue(
                    msg=f'consistent_sports returns False at least once',
                    expr=(False in consistent_sports_outputs))
                    
    def test_answer_yes(self):
        # if final answer is 'yes' then all outputs of sport_for have
        # been passed as an input to consistent_sports, and
        # consistent_sports has never returned False
        trace = self.trace
        if trace.final_answer.lower() == 'yes':
            with self.subTest(msg='answer is yes'):
                sport_for_outputs = trace.output_set('sport_for')
                consistent_sports_inputs = trace.flat_input_set('consistent_sports')
                self.assertTrue(
                    msg='all sport_for outputs are consistent_sports inputs',
                    expr=(sport_for_outputs <= consistent_sports_inputs)
                )
                consistent_sports_outputs = trace.output_set('consistent_sports')
                self.assertTrue(
                    msg=f'consistent_sports never returns False',
                    expr=(False not in consistent_sports_outputs))


class AuditDateUnderstanding(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)


    def test_single_steps(self):
        df = self.trace.df
        # each of the extract_options, extract_date_facts, extract_question, and answer_question
        # steps are only used once
        for fn in ['extract_options', 'extract_date_facts', 'extract_question', 'answer_question']:
            self.assertTrue(
                msg=f'one step with fn_name of "{fn}"',
                expr=(len(df[df.fn_name == fn]) == 1))

    def test_inferences(self):
        trace = self.trace
        df = trace.df
        # the number of date_facts is the same as the number of make_inference steps
        date_facts = trace.flat_output_list('extract_date_facts')
        self.assertTrue(
            msg=f'a make_inference step is called for each output of extract_date_facts',
            expr=(len(date_facts) == len(df[df.fn_name=='make_inference'])))

    def test_option_matching(self):
        trace = self.trace
        df = trace.df
        # the number of extracted options equals the number of
        # 'summary_matches_option' steps in the trace
        options = trace.flat_output_list('extract_options')
        self.assertTrue(
            msg=(f'the number of options equals the number of match_option steps'),
            expr=(len(df[df.fn_name=='match_option']) == len(options)))

AUDITOR_CLASS = {
    'sports_understanding': AuditSportsUnderstanding,
    'geometric_shapes': AuditGeometricShapes,
    'date_understanding': AuditDateUnderstanding,
    'medcalc_rules': AuditMedcalcRules,
}

class AuditRunner:

    def __init__(self, verbose=0, filter_parse_failed_examples=True):
        parser = arg_util.baseparser()
        self.args = parser.parse_args()
        if self.args.test_set:
            self.args.example_dir = '../doctest-prompting-data/data/test/'
        log_util.load_partialprogram(self.args)
        filename = arg_util.log_file(self.args)
        self.examples = log_util.load_examples(filename, verbose=True)
        if filter_parse_failed_examples:
            self.examples = [ex for ex in self.examples if not ex['y_hat'].startswith('**parse')]
        self.logged_traces = [ex['output'] for ex in self.examples]
        self.results = []
        
    def run(self, **auditor_class_kw):
        for i, logged_trace in enumerate(self.logged_traces):
            task = self.args.task
            c = AUDITOR_CLASS.get(task)
            if c is None:
                raise ValueError(f'no auditor class for {self.args.task}')
            else:
                a = c(logged_trace, i, **auditor_class_kw)
                a.run_audit()
                self.results.append(a)
        return self.results

    def __str__(self):
        num_traces = len(self.results)
        num_traces_with_failures = len([a for a in self.results if a.failures])
        num_traces_with_syntax_errors = len([a for a in self.results if a.parse_error is not None])
        total_audits = sum([len(a.audits) for a in self.results])
        total_failures = sum([len(a.failures) for a in self.results])
        return f'{num_traces=} {num_traces_with_failures=} {num_traces_with_syntax_errors=} {total_audits=} {total_failures=}'

def crosstabs(runner, result_filter_fn, do_tests=True):
    correctness = [
        ex['is_correct'] 
        for r, ex in zip(runner.results, runner.examples)
        if result_filter_fn(r)]
    num_failures = [
        len(r.failures)
        for r in runner.results
        if result_filter_fn(r)]
    df = pd.DataFrame(
        dict(num_failures=num_failures,
             correctness=correctness))
    xt = pd.crosstab(df.num_failures == 0, df.correctness)
    xtnp = xt.to_numpy()
    if do_tests and xtnp.shape == (2,2):
        print('fisher exact', fisher_exact(xtnp))
        print('binom test row 0', binomtest(k=xtnp[0,0], n=sum(xtnp[0,:])).proportion_ci())
        print('binom test row 1', binomtest(k=xtnp[1,0], n=sum(xtnp[1,:])).proportion_ci())
    return xt

if __name__ == "__main__":        
    runner = AuditRunner()
    results = runner.run()
    for a in results:
        print(a)
    print(runner)

    num_fails = [len(r.failures) for r in results]
    correctness = [ex['is_correct'] for ex in runner.examples]
    df = pd.DataFrame(dict(num_fails=num_fails, correctness=correctness))
    dfxt = pd.crosstab(df.num_fails == 0, df.correctness)
    xt = dfxt.to_numpy()
    print('-' * 20, 'overall crosstabs', '-' * 20)
    print(dfxt)
    if xt.shape==(2,2):
        print('no errors', binomtest(k=xt[0,0], n=sum(xt[0,:])).proportion_ci())
        print('with errors', binomtest(k=xt[1,0], n=sum(xt[1,:])).proportion_ci())

    signif_failures = {}
    failures = set(f for r in results for f in r.failures)
    for failure in failures:
        fails = [failure in r.failures for r in results]
        correctness = [ex['is_correct'] for ex in runner.examples]
        df = pd.DataFrame(dict(fails=fails, correctness=correctness))
        dfxt = pd.crosstab(df.fails, df.correctness)
        print('-' * 20, 'crosstabs for', failure, '-' * 20)
        print(dfxt)
        xt = dfxt.to_numpy()
        if xt.shape==(2,2):
            fe = fisher_exact(xt)
            ci0 = binomtest(k=xt[0,0], n=sum(xt[0,:])).proportion_ci()
            ci1 = binomtest(k=xt[1,0], n=sum(xt[1,:])).proportion_ci()
            if ci0.high < ci1.low or fe.pvalue <= 0.05:
                signif_failures[failure] = (fe, xt)

    print(f'{len(signif_failures)} statistically significant failures')
    pprint.pprint(signif_failures)

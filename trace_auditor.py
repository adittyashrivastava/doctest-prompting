import collections
from contextlib import contextmanager
import dataclasses
import pandas as pd
import pprint
import unittest
from typing import Any

import arg_util
import debugger
import log_util

Global_traces = None

class ParsedTrace:
    """A parsed PTP trace.

    Attributes are final_answer and df, where df is a Pandas DataFrame
    with keys 
     - fn_name (the 'function name' of the step
     - inputs (a tuple of input values)
     - output (the single output of a step),
     - start_line, end_line (the position of the step in the trace)
    """

    def __init__(self, logged_trace):
        self.final_answer = None
        for line in logged_trace:
            if line.startswith('Final answer: '):
                self.final_answer = line[len('Final answer: '):].rstrip()

        def eval_args(so):
            inps, had_error = log_util.safe_eval(so.args, is_input=True)
            if had_error:
                raise ValueError(f'value syntax/type error: {so.start_line} {so.args}')
            return inps

        def eval_return_value(so):
            return_value, had_error = log_util.safe_eval(so.value)
            if had_error:
                raise ValueError(f'value syntax/type error: {so.end_line}: {so.value}')
            return return_value

        step_outputs = [
            so 
            for so in debugger.outputs_of(None, logged_trace, raise_error=True)]
        self.df = pd.DataFrame(
            dict(inputs=[eval_args(so) for so in step_outputs],
                 output=[eval_return_value(so) for so in step_outputs],
                 fn_name=[so.fn_name for so in step_outputs],
                 start_line=[so.start_line for so in step_outputs],
                 end_line=[so.end_line for so in step_outputs])
            )

    def flat_input_set(self, step_name):
        """A set derived from a flattened list of all inputs to steps
        with the given name.
        """
        return set([inp
                    for step in self.df[self.df.fn_name == step_name].itertuples()
                    for inp in step.inputs])

    def output_set(self, step_name):
        """A set derived from a list of all outputs to steps with the
        given name.
        """
        return set([step.output
                    for step in self.df[self.df.fn_name == step_name].itertuples()])

    def flat_output_set(self, step_name):
        """A set derived from a flattened list of all outputs to steps
        with the given name.

        This will only work for steps that return tuples as outputs.
        """
        return set([outp 
                    for step in self.df[self.df.fn_name=='analyze_sentence'].itertuples()
                    for outp in step.output])

class Auditor:
    """Subclass this to create a task-specific auditor.

    Attributes are
    - audits: list of audit messages for audits that have been run
    - failures: list of messages for failed audits
    - parse_error: None or an Exception.  If an Exception then no audits will be run.
    - index, logged_trace: the trace that was audited and its index in the log file.
    """

    def __init__(self, logged_trace, index):
        self.subaudits = [self.__class__.__name__]
        self.audits = []
        self.failures = []
        self.index = index
        self.logged_trace = logged_trace
        try:
            self.trace = ParsedTrace(logged_trace)
            self.parse_error = None
        except ValueError as ex:
            self.trace = None
            self.parse_error = ex

    @contextmanager
    def subaudit(self, msg):
        self.subaudits.append(msg)
        yield
        self.subaudits.pop()

    def run_audit(self):
        """Run all audit methods for the trace.  Audit methods are
        methods starting with 'audit_'.
        """
        if self.trace is not None:
            for method_name, method in self.__class__.__dict__.items():
                if method_name.startswith('audit_'):
                    method(self)

    def confirm(self, msg, condition):
        """Confirm that the condition holds, and record the message as
        a completed audit in self.audits.

        If the condition is false, also record the msg as a failed audit
        is self.failures.
        """
        augmented_msg = '.'.join(self.subaudits + [msg])
        self.audits.append(augmented_msg)
        if not condition:
            self.failures.append(augmented_msg)

    def __str__(self):
        return f'audits: {len(self.audits)} failed: {len(self.failures)} syntax errors: {self.parse_error!r}'

#
# task specific auditors
#

class AuditGeometricShapes(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)

    def audit_single_steps(self):
        trace = self.trace
        for fn in ['extract_path', 'decompose_path',
                   'summarize_decomposed_path', 'extract_options']:
            self.confirm(
                f'one step with fn_name of "{fn}"',
                len(trace.df[trace.df.fn_name==fn]) == 1)
    
    def audit_option_matching(self):
        trace = self.trace
        df = trace.df
        extract_options_step, *_ = df[df.fn_name == 'extract_options'].itertuples()
        options = extract_options_step.output
        self.confirm(
            (f'the number of options ({len(options)}) equals the number of ' +
             f'summary_matches_option steps'),
            len(df[df.fn_name=='summary_matches_option']) == len(options))

    def audit_length_clusters(self):
        trace = self.trace
        df = trace.df
        matching_option_outputs = df[df.fn_name=='summary_matches_option'].output.tolist()
        ctr = collections.Counter(matching_option_outputs)
        if ctr[True] == 1:
            with self.subaudit('one matching option'):
                self.confirm(
                    'no compute_length_clusters steps',
                    len(df[df.fn_name=='compute_length_clusters']) == 0)
                self.confirm(
                    'no relate_length_clusters_to_option steps',
                    len(df[df.fn_name=='relate_length_clusters_to_option']) == 0)
        if ctr[True] > 1:
            with self.subaudit('multiple matching options'):
                self.confirm(
                    'one compute_length_clusters step',
                    len(df[df.fn_name=='compute_length_clusters']) == 1)
                # model should then check each option against the length clusters
                matching_option_inputs = df[df.fn_name=='summary_matches_option'][df.output == True].inputs.tolist()
                matching_options = [inp[1] for inp in matching_option_inputs]
                self.confirm(
                    (f'the number of matching options ({len(matching_options)}) equals the number of ' +
                     f'relate_length_clusters_to_option steps'),
                    len(matching_options) == len(df[df.fn_name=='relate_length_clusters_to_option']))


class AuditSportsUnderstanding(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)

    def audit_analyze_sentence_step(self):
        trace = self.trace
        df = trace.df
        self.confirm(
            'there is one step with fn_name of "analyze_sentence"',
            len(df[df.fn_name=='analyze_sentence']) == 1)

        analyze_sentence_outputs = [
            outp for outp in trace.flat_output_set('analyze_sentence') if outp != '']
        self.confirm(
            'the analyze_sentence step has 2-3 outputs',
            2 <= len(analyze_sentence_outputs) <= 3)

    def audit_sport_for_inputs(self):
        # all inputs to sport_for are outputs of analyze_sentence
        trace = self.trace
        analyze_sentence_outputs = trace.flat_output_set('analyze_sentence')
        sport_for_inputs = trace.flat_input_set('sport_for')
        for x in sport_for_inputs:
            self.confirm(
                f'input to sport_for {x!r} is output of analyze_sentence',
                x in analyze_sentence_outputs)

    def audit_consistent_sport_inputs(self):
        # all inputs to consistent_sports are outputs of sport_for
        trace = self.trace
        sport_for_outputs = trace.output_set('sport_for')
        consistent_sports_inputs = trace.flat_input_set('consistent_sports')
        for x in consistent_sports_inputs:
            self.confirm(
                f'input to consistent_sports {x!r} is output of sport_for',
                x in sport_for_outputs)

    def audit_answer_no(self):
        # if final answer is not 'yes' then something is inconsistent        
        trace = self.trace
        if trace.final_answer.lower() != 'yes':
            consistent_sports_outputs = trace.output_set('consistent_sports')
            self.confirm(
                f'final answer is not "yes" and something is inconsistent',
                False in consistent_sports_outputs)
                    
    def audit_answer_yes(self):
        # if final answer is 'yes' then all sports have been checked
        # and none of them are inconsistent 
        trace = self.trace
        if trace.final_answer.lower() == 'yes':
            sport_for_outputs = trace.output_set('sport_for')
            consistent_sports_inputs = trace.flat_input_set('consistent_sports')
            for sport in sport_for_outputs:
                self.confirm(
                    f'final answer is "yes" and {sport!r} was checked for consistent_sports',
                    sport in consistent_sports_inputs)
            consistent_sports_outputs = trace.output_set('consistent_sports')
            self.confirm(
                f'final answer is "yes" and nothing is in consistent_sports',
                False not in consistent_sports_outputs)


AUDITOR_CLASS = {
    'sports_understanding': AuditSportsUnderstanding,
    'geometric_shapes': AuditGeometricShapes,
}

class AuditRunner:

    def __init__(self, verbose=0):
        parser = arg_util.baseparser()
        self.args = parser.parse_args()
        if self.args.test_set:
            self.args.example_dir = '../doctest-prompting-data/data/test/'
        log_util.load_partialprogram(self.args)
        filename = arg_util.log_file(self.args)
        self.examples = log_util.load_examples(filename, verbose=True)
        self.logged_traces = [ex['output'] for ex in self.examples]
        
    def run(self):
        results = []
        for i, logged_trace in enumerate(self.logged_traces):
            c = AUDITOR_CLASS.get(self.args.task)
            if c is None:
                raise ValueError(f'no auditor class for {self.args.task}')
            else:
                a = c(logged_trace, i)
                a.run_audit()
                results.append(a)
        return results


if __name__ == "__main__":        
    runner = AuditRunner()
    results = runner.run()
    for a in results:
        print(a)

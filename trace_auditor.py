import collections
from contextlib import contextmanager
import dataclasses
import pandas as pd
import pprint
import re
import unittest

from scipy.stats import binomtest
from scipy.stats import fisher_exact

import arg_util
import debugger
import log_util

def expand(compressed_trace):
    binding = {}
    for line in compressed_trace:
        m = re.match('^(v\d+) = (.*)$', line)
        if m:
            binding[m.group(1)] = m.group(2)
    expanded = []
    failures = []
    for line in compressed_trace:
        if line.startswith('Calling') or line.startswith('...'):
            vars = re.findall('(v\d+)', line)
            for v in vars:
                if v not in binding:
                    failures.append(f'use of unbound variable {v}')
                    binding[v] = f'"**unbound variable {v}**"'
                line = line.replace(v, binding[v])
            expanded.append(line)
    return expanded, failures

class ParsedTrace:
    """A parsed PTP trace.

    Attributes are final_answer and df, where df is a Pandas DataFrame
    with keys 
     - fn_name (the 'function name' of the step
     - inputs (a tuple of input values)
     - output (the single output of a step),
     - start_line, end_line (the position of the step in the trace)
    """

    def __init__(self, logged_trace,
                 robust_parse=True, eval_args=True, eval_return_value=True):
        self.final_answer = None
        self.robust_parse = robust_parse
        self.eval_args = eval_args
        self.eval_return_value = eval_return_value
        for line in logged_trace:
            if line.startswith('Final answer: '):
                self.final_answer = line[len('Final answer: '):].rstrip()
        if self.robust_parse:
            logged_trace = self.cleanup(logged_trace)
            logged_trace = [line.lstrip() for line in logged_trace]
        step_outputs = [
            so 
            for so in debugger.outputs_of(None, logged_trace, raise_error=True)]
        def get_args(so):
            return self.eval_step_args(so) if self.eval_args else so.args
        def get_return_value(so):
            return self.eval_step_return_value(so) if self.eval_return_value else so.args
        self.df = pd.DataFrame(
            dict(inputs=[get_args(so) for so in step_outputs],
                 output=[get_return_value(so) for so in step_outputs],
                 fn_name=[so.fn_name for so in step_outputs],
                 start_line=[so.start_line for so in step_outputs],
                 end_line=[so.end_line for so in step_outputs])
            )

    def robust_eval(self, so, to_eval, is_input):
        result, had_error = log_util.safe_eval(to_eval, is_input=is_input)
        if had_error:
            if self.robust_parse:
                # try again without single quotes inside the string, a
                # common problem
                to_eval = re.sub("(\w)'(\w)", "\1\\'\2", to_eval)
                result, had_error = log_util.safe_eval(to_eval, is_input=is_input)
            if had_error:
                where = {True:'args', False:'return'}
                raise ValueError(f'eval error for {where[is_input]} line {so.start_line}: <{so.args[0:340]}>')
        return result

    def eval_step_args(self, so):
        return self.robust_eval(so, so.args, is_input=True)

    def eval_step_return_value(self, so):
        return self.robust_eval(so, so.value, is_input=False)

    def cleanup(self, logged_trace):
        """Collapse step calls/returns that span multiple lines.
        """
        clean_trace = []
        logged_trace = [line.lstrip() for line in logged_trace]
        i = 0
        while i < len(logged_trace):
            buffer = logged_trace[i]
            m = re.match('^\s*Calling (\w+)\(', buffer)
            if m is not None:
                called_fn = m.group(1)
                while (i + 1 < len(logged_trace)
                       and not logged_trace[i + 1].startswith(f'...{called_fn} returned ')):
                    buffer = buffer.rstrip() + ' ' + logged_trace[i + 1]
                    i += 1
            clean_trace.append(buffer)
            i += 1
        return clean_trace

    def flat_input_set(self, step_name):
        """A set derived from a flattened list of all inputs to steps
        with the given name.
        """
        return set([inp
                    for step in self.df[self.df.fn_name == step_name].itertuples()
                    for inp in step.inputs])

    def output_set(self, step_name):
        """A set derived from a list of all outputs to steps with the
        given name.
        """
        return set([step.output
                    for step in self.df[self.df.fn_name == step_name].itertuples()])

    def flat_output_list(self, step_name):
        """A flattened list of all outputs to steps with the given
        name.

        This will only work for steps that return tuples as outputs.

        """
        try:
            return [outp 
                    for step in self.df[self.df.fn_name==step_name].itertuples()
                    for outp in step.output]
        except TypeError as ex:
            print(f'error in flat_output_list {ex}')
            return []

class Auditor:
    """Subclass this to create a task-specific auditor.

    Designed to look like unittest.TestCase instances.

    Attributes are
    - audits: list of audit messages for audits that have been run
    - failures: list of messages for failed audits
    - parse_error: None or an Exception.  If an Exception then no audits will be run.
    - index, logged_trace: the trace that was audited and its index in the log file.
    """

    def __init__(self, logged_trace, index, **kw):
        self.sub_audits = [self.__class__.__name__]
        self.audits = []
        self.failures = []
        self.index = index
        self.logged_trace = logged_trace
        try:
            self.trace = ParsedTrace(logged_trace, **kw)
            self.parse_error = None
        except ValueError as ex:
            self.trace = None
            self.parse_error = ex

    @contextmanager
    def subTest(self, msg, **params):
        self.sub_audits.append(msg)
        yield
        self.sub_audits.pop()

    def assertTrue(self, expr, msg):
        """Confirm that the condition holds, and record the message as
        a completed audit in self.audits.

        If the condition is false, also record the msg as a failed audit
        is self.failures.
        """
        augmented_msg = '.'.join(self.sub_audits + [msg])
        self.audits.append(augmented_msg)
        if not expr:
            self.failures.append(augmented_msg)

    def run_audit(self):
        """Run all audit methods for the trace.  Audit methods are
        methods starting with 'audit_'.
        """
        if self.trace is not None:
            for method_name, method in self.__class__.__dict__.items():
                if method_name.startswith('test_'):
                    method(self)

    def __str__(self):
        return f'audits: {len(self.audits)} failed: {len(self.failures)} syntax errors: {self.parse_error!r}'

#
# task specific auditors
#

class AuditMedcalcRules(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)

    def test_analyze_input(self):
        trace = self.trace
        self.assertTrue(
            msg=f'one step with fn_name of "analyze_input"',
            expr=(len(trace.df[trace.df.fn_name == 'analyze_input']) == 1))
        
    def test_each_rule_applied(self):
        trace = self.trace
        analysis_parts = trace.flat_output_list('analyze_input')
        self.assertTrue(
            msg=f'analyze_input returns three values',
            expr=(len(analysis_parts) == 3))
        if len(analysis_parts) == 3:
            _, rules, _ = analysis_parts
            num_rules = len(rules)
            for fn_name in ['get_data', 'convert_units', 'check_rule', 'accumulate_score']:
                self.assertTrue(
                    msg=f'one step per rule with fn_name of "{fn_name}"',
                    expr=(len(trace.df[trace.df.fn_name == fn_name]) == num_rules))
            

class AuditMedcalcRulesNoteless(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)

    def test_analyze_input(self):
        trace = self.trace
        self.assertTrue(
            msg=f'one step with fn_name of "analyze_input"',
            expr=(len(trace.df[trace.df.fn_name == 'analyze_input']) == 1))
        
    def test_each_rule_applied(self):
        trace = self.trace
        analysis_parts = trace.flat_output_list('analyze_input')
        self.assertTrue(
            msg=f'analyze_input returns two values',
            expr=(len(analysis_parts) == 2))
        if len(analysis_parts) == 2:
            _, rules = analysis_parts
            num_rules = len(rules)
            for fn_name in ['get_data', 'convert_units', 'check_rule', 'accumulate_score']:
                self.assertTrue(
                    msg=f'one step per rule with fn_name of "{fn_name}"',
                    expr=(len(trace.df[trace.df.fn_name == fn_name]) == num_rules))

class AuditGeometricShapes(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)

    def test_single_steps(self):
        trace = self.trace
        # each of the extract_path, decompose_path,
        # summarize_decomposed_path, and extract_options steps are
        # only used once
        for fn in ['extract_path', 'decompose_path',
                   'summarize_decomposed_path', 'extract_options']:
            self.assertTrue(
                msg=f'one step with fn_name of "{fn}"',
                expr=(len(trace.df[trace.df.fn_name == fn]) == 1))
    
    def test_option_matching(self):
        trace = self.trace
        df = trace.df
        # the number of extracted options equals the number of
        # 'summary_matches_option' steps in the trace
        extract_options_step, *_ = df[df.fn_name == 'extract_options'].itertuples()
        options = extract_options_step.output
        self.assertTrue(
            msg=(f'the number of options ({len(options)}) equals the number of ' +
                 f'summary_matches_option steps'),
            expr=(len(df[df.fn_name=='summary_matches_option']) == len(options)))

    def test_length_clusters(self):
        trace = self.trace
        df = trace.df
        matching_option_outputs = df[df.fn_name=='summary_matches_option'].output.tolist()
        ctr = collections.Counter(matching_option_outputs)
        # if only one summary_matches_option step returns True, then
        # there should be no calls to the compute_length_clusters or
        # relate_length_clusters_to_option steps
        if ctr[True] == 1:
            with self.subTest(msg='one matching option'):
                self.assertTrue(
                    msg='no compute_length_clusters steps',
                    expr=(len(df[df.fn_name=='compute_length_clusters']) == 0))
                self.assertTrue(
                    msg='no relate_length_clusters_to_option steps',
                    expr=(len(df[df.fn_name=='relate_length_clusters_to_option']) == 0))
        # if more than one summary_matches_option step returns True,
        # then there should be one calls to the compute_length_clusters,
        # and one call to relate_length_clusters_to_option for each
        # option for which summary_matches_option returned True
        if ctr[True] > 1:
            with self.subTest(msg='multiple matching options'):
                self.assertTrue(
                    msg='one compute_length_clusters step',
                    expr=(len(df[df.fn_name=='compute_length_clusters']) == 1))
                # model should then check each option against the length clusters
                matching_option_inputs = df[df.fn_name=='summary_matches_option'][df.output == True].inputs.tolist()
                matching_options = [inp[1] for inp in matching_option_inputs]
                self.assertTrue(
                    msg=(f'the number of matching options ({len(matching_options)}) equals the number of ' +
                         f'relate_length_clusters_to_option steps'),
                    expr=(len(matching_options) == len(df[df.fn_name=='relate_length_clusters_to_option'])))


class AuditSportsUnderstanding(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)

    def test_analyze_sentence_step(self):
        # there should be one analyze_sentence step, which returns
        # either 2 or 3 non-empty-string values
        trace = self.trace
        df = trace.df
        self.assertTrue(
            msg='there is one step with fn_name of "analyze_sentence"',
            expr=(len(df[df.fn_name=='analyze_sentence']) == 1))
        analyze_sentence_outputs = [
            outp for outp in set(trace.flat_output_list('analyze_sentence')) if outp != '']
        self.assertTrue(
            msg='the analyze_sentence step has 2-3 outputs',
            expr=(2 <= len(analyze_sentence_outputs) <= 3))

    def test_sport_for_inputs(self):
        # all inputs to sport_for are outputs of analyze_sentence
        trace = self.trace
        analyze_sentence_outputs = set(trace.flat_output_list('analyze_sentence'))
        sport_for_inputs = trace.flat_input_set('sport_for')
        self.assertTrue(
            msg=f'every input to sport_for output of analyze_sentence',
            expr=(sport_for_inputs <= analyze_sentence_outputs))

    def test_consistent_sport_inputs(self):
        # all inputs to consistent_sports are outputs of sport_for
        trace = self.trace
        sport_for_outputs = trace.output_set('sport_for')
        consistent_sports_inputs = trace.flat_input_set('consistent_sports')
        self.assertTrue(
            msg=f'every input to consistent_sports output of sport_for',
            expr=(consistent_sports_inputs <= sport_for_outputs))

    def test_answer_no(self):
        # if final answer is not 'yes' then some call to
        # consistent_sports returns False
        trace = self.trace
        if trace.final_answer.lower() != 'yes':
            with self.subTest(msg='answer is no'):
                consistent_sports_outputs = trace.output_set('consistent_sports')
                self.assertTrue(
                    msg=f'consistent_sports returns False at least once',
                    expr=(False in consistent_sports_outputs))
                    
    def test_answer_yes(self):
        # if final answer is 'yes' then all outputs of sport_for have
        # been passed as an input to consistent_sports, and
        # consistent_sports has never returned False
        trace = self.trace
        if trace.final_answer.lower() == 'yes':
            with self.subTest(msg='answer is yes'):
                sport_for_outputs = trace.output_set('sport_for')
                consistent_sports_inputs = trace.flat_input_set('consistent_sports')
                self.assertTrue(
                    msg='all sport_for outputs are consistent_sports inputs',
                    expr=(sport_for_outputs <= consistent_sports_inputs)
                )
                consistent_sports_outputs = trace.output_set('consistent_sports')
                self.assertTrue(
                    msg=f'consistent_sports never returns False',
                    expr=(False not in consistent_sports_outputs))


class AuditDateUnderstanding(Auditor):

    def __init__(self, logged_trace, i):
        super().__init__(logged_trace, i)


    def test_single_steps(self):
        df = self.trace.df
        # each of the extract_options, extract_date_facts, extract_question, and answer_question
        # steps are only used once
        for fn in ['extract_options', 'extract_date_facts', 'extract_question', 'answer_question']:
            self.assertTrue(
                msg=f'one step with fn_name of "{fn}"',
                expr=(len(df[df.fn_name == fn]) == 1))

    def test_inferences(self):
        trace = self.trace
        df = trace.df
        # the number of date_facts is the same as the number of make_inference steps
        date_facts = trace.flat_output_list('extract_date_facts')
        self.assertTrue(
            msg=f'a make_inference step is called for each output of extract_date_facts',
            expr=(len(date_facts) == len(df[df.fn_name=='make_inference'])))

    def test_option_matching(self):
        trace = self.trace
        df = trace.df
        # the number of extracted options equals the number of
        # 'summary_matches_option' steps in the trace
        options = trace.flat_output_list('extract_options')
        self.assertTrue(
            msg=(f'the number of options equals the number of match_option steps'),
            expr=(len(df[df.fn_name=='match_option']) == len(options)))

AUDITOR_CLASS = {
    'sports_understanding': AuditSportsUnderstanding,
    'geometric_shapes': AuditGeometricShapes,
    'date_understanding': AuditDateUnderstanding,
    'medcalc_rules': AuditMedcalcRules,
    'medcalc_rules_noteless': AuditMedcalcRulesNoteless,
}

class AuditRunner:

    def __init__(self, verbose=0, filter_parse_failed_examples=True):
        parser = arg_util.baseparser()
        self.args = parser.parse_args()
        self.task = self.args.task + self.args.variant
        if self.args.test_set:
            self.args.example_dir = '../doctest-prompting-data/data/test/'
        log_util.load_partialprogram(self.args)
        filename = arg_util.log_file(self.args)
        self.examples = log_util.load_examples(filename, verbose=True)
        if filter_parse_failed_examples:
            self.examples = [ex for ex in self.examples if not ex['y_hat'].startswith('**parse')]
        self.logged_traces = [ex['output'] for ex in self.examples]
        self.expand_traces = '_c' in self.args.variant
        self.results = []
        
    def run(self, **auditor_class_kw):
        for i, logged_trace in enumerate(self.logged_traces):
            task = self.task
            c = AUDITOR_CLASS.get(task)
            if c is None:
                raise ValueError(f'no auditor class for {task}')
            if self.expand_traces:
                logged_trace, expansion_failures = expand(logged_trace)
            else:
                expansion_failures = []
            a = c(logged_trace, i, **auditor_class_kw)
            a.failures.extend(expansion_failures)
            a.run_audit()
            self.results.append(a)
        return self.results

    def __str__(self):
        num_traces = len(self.results)
        num_traces_with_failures = len([a for a in self.results if a.failures])
        num_traces_with_syntax_errors = len([a for a in self.results if a.parse_error is not None])
        total_audits = sum([len(a.audits) for a in self.results])
        total_failures = sum([len(a.failures) for a in self.results])
        return f'{num_traces=} {num_traces_with_failures=} {num_traces_with_syntax_errors=} {total_audits=} {total_failures=}'

class LogAuditRunner(AuditRunner):
    def __init__(self, log_filename, filter_parse_failed_examples=True, compact_traces=False):
        self.examples = log_util.load_examples(log_filename, verbose=oTrue)
        if filter_parse_failed_examples:
            self.examples = [ex for ex in self.examples if not ex['y_hat'].startswith('**parse')]
        self.logged_traces = [ex['output'] for ex in self.examples]
        self.expand_traces = compact_traces
        self.results = []


def crosstabs(runner, result_filter_fn, do_tests=True):
    is_correct = [
        ex['is_correct'] 
        for r, ex in zip(runner.results, runner.examples)
    ]
    passed_filter = [
        result_filter_fn(r)
        for r in runner.results
    ]
    df = pd.DataFrame(
        dict(passed_filter=passed_filter,
             is_correct=is_correct))
    xt = pd.crosstab(df.passed_filter, df.is_correct)
    xtnp = xt.to_numpy()
    if xtnp.shape == (2,2):
        fe = fisher_exact(xtnp)
        if do_tests:
            print('fisher exact', fe)
            print('binom test row 0', binomtest(k=xtnp[0,0], n=sum(xtnp[0,:])).proportion_ci())
            print('binom test row 1', binomtest(k=xtnp[1,0], n=sum(xtnp[1,:])).proportion_ci())
        return xt, fe.pvalue
    else:
        return xt, 1.0

def skeleton(result): 
    if result.trace is None:
        return None
    return '.'.join(result.trace.df.fn_name)

def skeleton_frequency_score(runner):
    ctr = collections.Counter([skeleton(r) for r in runner.results])
    ctr[None] = ctr[''] = -1
    def scorer(runner, i):
        return ctr[skeleton(runner.results[i])]
    return scorer

def analysis_by_linear_score(runner, scoring_fn, reverse=True, score_filter=None):
    # score and correctness of examples
    if score_filter is None:
        score_filter = lambda score: True
    correctness = [int(ex['is_correct']) for ex in runner.examples]
    scores = [
        scoring_fn(runner, i)
        for i in range(len(runner.examples))]
    # indices of examples sorted by score
    indices_by_score = sorted(
        range(len(runner.examples)),
        key=lambda i: scores[i],
        reverse=reverse)

    indices_by_score = [j for j in indices_by_score if score_filter(scores[j])]
    j0 = indices_by_score[0]
    last_score = scores[j0]
    n_total = slice_n_total = 1
    n_correct = slice_n_correct = correctness[j0]
    curve = []
    for j in indices_by_score[1:]:
        if scores[j] != last_score:
            curve.append((last_score, n_correct, n_total, slice_n_correct, slice_n_total))
            last_score = scores[j]
            slice_n_total = slice_n_correct = 0
        n_correct += correctness[j]
        slice_n_correct += correctness[j]
        n_total += 1
        slice_n_total += 1
    curve.append((last_score, n_correct, n_total, slice_n_correct, slice_n_total))
    return curve

def print_curve(curve):
    _, tot_n_correct, tot_n_total,_, _ = curve[-1]
    baseline_p = tot_n_correct/tot_n_total
    for last_score, n_correct, n_total, slice_n_correct, slice_n_total in curve:
        cum_bt = binomtest(k=n_correct, n=n_total, p=baseline_p)
        slice_bt = binomtest(k=slice_n_correct, n=slice_n_total, p=baseline_p)
        print(f'{last_score}\t{n_correct}/{n_total}\t{cum_bt.statistic:.2f}\t{cum_bt.pvalue:.4f}' +
              f'\t{slice_bt.statistic:.2f}\t{slice_bt.pvalue:.4f}')

if __name__ == "__main__":        
    raise ValueError("deprecated")

    runner = AuditRunner()
    results = runner.run()

    print('crosstabs for any failure')
    xt, _ = crosstabs(runner, lambda r:bool(r.failures), do_tests=True)
    print(xt)

    print('crosstabs for parse failure')
    xt, _ = crosstabs(runner, lambda r:bool(r.parse_error), do_tests=True)
    print(xt)

    failures = set(f for r in results for f in r.failures)
    signif_failures = {}
    min_pvalue_failure = (1.1, None, None)
    for failure in failures:
        xt, pvalue = crosstabs(runner, lambda r:failure in r.failures, do_tests=False)
        if pvalue < 0.1:
            print('crosstabs for failure', failure)
            print(xt)
            signif_failures[failure] = (pvalue, xt)
        if pvalue < min_pvalue_failure[0]:
            min_pvalue_failure = (pvalue, failure, xt)
    print(f'{len(signif_failures)} statistically significant failures')
    pprint.pprint(signif_failures)
    print('min pvalue failure')
    pprint.pprint(min_pvalue_failure)

    skeleton_freq = skeleton_frequency_score(runner)
    pos_score = lambda score: score > 0
    curve = analysis_by_linear_score(runner, skeleton_freq, reverse=True, score_filter=pos_score)
    print('\t'.join(["skfreq", "cum acc", "pval", "acc", "pval"]))
    print_curve(curve)
    curve = analysis_by_linear_score(runner, skeleton_freq, reverse=False, score_filter=pos_score)
    print('\t'.join(["skfreq", "cum acc", "pval", "acc", "pval"]))
    print_curve(curve)
